# 网络RPN及最后预测输出了啥

考虑最后输出的预测值都是多少维度的，假设输入（Batch，H，W，C）C一般等于3

过去好久了，都忘了，猜一下，分类的应该是（R，N）R为框的总个数，N为数据集类别数。

边框因该是，（R，4）R为框的个数，4表示~~[ymin，xmin，ymax，xmin]~~，

应该表示预测的偏移量[dx，dy，dw，dh] 顺序我不清楚。

![边框偏移](C:\ZZZ\notes\assert\20181207112311329.PNG)

不知道猜的对不对。。。好像不对

## RPN网络

参考：https://www.cnblogs.com/king-lps/p/8981222.html

好像是每次只训练一张图片，batchsize = 1，RPN得到的特征即输入的特征维度为（1，512，H，W），如下图：

<img src="C:\ZZZ\notes\assert\1055519-20180503204713041-690764007.png" alt="RPN网络" style="zoom: 67%;" />

**1）RPN网络：**

训练自己：二分类、bounding box 回归（由AnchorTargetCreator实现）

提供rois：为Fast-RCNN提供训练需要的rois（由ProposalCreator实现）

左侧18，H×W每个点生成9个anchor，每个anchor二分类，右侧通路36，每个anchor，预测4个偏移值。

**2）RPN网络中AnchorTargetCreator分析：**

>目的：利用每张图中bbox的真实标签来为所有任务分配ground truth！
>
>输入：最初生成的20000个anchor坐标、此一张图中所有的bbox的真实坐标（可以打标签了）
>
>输出：size为（20000，1）的正负label（其中只有128个为1，128个为0，其余都为-1）、 size为（20000，4）的回归目标（所有anchor的坐标都有）

将20000多个候选的anchor选出**256个**anchor进行二分类和所有（这里有疑问？是所有还是几个，文章中Nreg  ≈ 2400？）的**anchor进行**回归位置 。为上面的**预测**值提供相应的**真实值**。选择方式如下：

- 对于每一个ground truth bounding box (`gt_bbox`)，选择和它重叠度（IoU）最高的一个anchor作为**正样本**。
- 对于剩下的anchor，从中选择和任意一个`gt_bbox`重叠度超过0.7的anchor，作为**正样本**，正样本的数目不超过128个。
- 随机选择和`gt_bbox`重叠度小于0.3的anchor作为**负样本**。负样本和正样本的总数为256。

对于每个anchor, gt_label 要么为1（前景），要么为0（背景），所以这样实现二分类。在计算回归损失的时候，只计算正样本（前景）的损失，不计算负样本的位置损失。

从代码角度看：

输入：20000个anchor[N,4]，bbox的标签（类别[R,]，坐标[R,4]）

筛选出完成包含在图像中的anchor，(0,0)<左上角 && （h,w)>右下角。此处可用mask数组记录，或者记录下这部分的索引数组（np.where）。假设有15000个满足条件.

```python
inside_index = _get_inside_index(anchor, img_H, img_W)
anchor = anchor[inside_index] # 第二种方法
```

然后再计算15000个anchor与真实ground_truth:(R,4)的IoUs，得到（15000，R）维度的IoU值的数组。（代码实际计算中，应该计算所有anchor（大约20000个）与真实框的IoU，毕竟这样写简便点，但计算量大了）。

人家用的是第二种方法，返回前根据索引数组inside_index从15000映射回20000长度。

现在，通过IoU数组，可以标记每个anchor的对应的 真实框和类别(记录索引即可，记录成数组：框index[N,]，类别label[N,])。注意这里虽然是挑选出256个anchor，但【最终】计算损失时仍然都是长度为N的数组。

```python
argmax_ious, label = self._create_label(inside_index, anchor, bbox) 
# anchor 与 bbox可以计算IoU数组，再通过IoU数组打标签
# 怎么打是个技术活
```



类别可以用于回归了，但得到的真实框的坐标并不是用于回归的目标，真正需要的是：ground_truth框与anchor的偏移量。

```python
# anchor:[N,4] N ≈ 20000
# bbox ground truth：[R,4] ==框index[N,]==>每个anchor对应的真实框: [N,4]
# 只计算256 mask标记的 或者 全部计算？

# 得到用于偏移的ground truth：[N,4]
```

最终，有了RPN网络两个1×1卷积输出的类别label和位置参数loc的预测值，AnchorTargetCreator又为其对应生成了真实值ground truth。那么AnchorTargetCreator的损失函数rpn_loss就能够计算了：



<img src="C:\ZZZ\notes\assert\20181207125458951.png" alt="在这里插入图片描述" style="zoom: 80%;" />

**3) RPN网络中ProposalCreator分析：**

只是利用预测的类别(两类)和偏移位置来给出RoIs区域。

RPN利用 **AnchorTargetCreator**自身训练的同时，还会提供RoIs（region of interests）给Fast RCNN（RoIHead）作为训练样本。RPN生成RoIs的过程(**ProposalCreator**)如下：

- 对于每张图片，利用它的feature map， 计算 (H/16)× (W/16)×9（大概20000）个anchor属于前景的概率，以及对应的位置参数。
- 选取概率较大的12000个anchor
- 利用回归的位置参数，**修正**这12000个anchor的位置，得到RoIs
- 利用非极大值（(Non-maximum suppression, NMS）抑制，选出概率最大的**2000个RoIs**

注意：在inference的时候，为了提高处理速度，12000和2000分别变为6000和300.

注意：这部分的操作不需要进行反向传播，因此可以利用numpy/tensor实现。

**RPN的输出：RoIs（形如2000×4或者300×4的tensor）**

## RoIHead网络

<img src="C:\ZZZ\notes\assert\1055519-20180503204921875-413753504.png" alt="head" style="zoom: 67%;" />

**ProposalTargetCreator**分析：

> 目的：为2000个rois赋予ground truth！（严格讲挑出128个赋予ground truth！）
>
> 输入：2000个rois、一个batch（一张图）中所有的bbox ground truth（R，4）、对应bbox所包含的label（R，1）（VOC2007来说20类0-19）
>
> 输出：128个sample roi（128，4）、128个gt_roi_loc（128，4）、128个gt_roi_label（128，1）

ProposalTargetCreator是RPN网络与ROIHead网络的过渡操作，前面讲过，RPN会产生大约2000个RoIs，这2000个RoIs不是都拿去训练，而是利用**ProposalTargetCreator** 选择128个RoIs用以训练。选择的规则如下：

- RoIs和gt_bboxes 的IoU大于0.5的，选择一些（比如32个）
- 选择 RoIs和gt_bboxes的IoU小于等于0（或者0.1）的选择一些（比如 128-32=96个）作为负样本

为了便于训练，对选择出的128个RoIs，还对他们的`gt_roi_loc` 进行标准化处理（减去均值除以标准差）

对于分类问题,直接利用交叉熵损失. 而对于位置的回归损失,一样采用Smooth_L1Loss, 只不过只对正样本计算损失，而且是只对正样本中的这个类别4个参数计算损失。举例来说:

- 一个RoI在经过FC 84后会输出一个84维的loc 向量. 如果这个RoI是负样本,则这84维向量不参与计算 L1_Loss
- 如果这个RoI是正样本,属于label K,那么它的第 K×4, K×4+1 ，K×4+2， K×4+3 这4个数参与计算损失，其余的不参与计算损失。

 

## 为啥位置预测是84维

每个类别21都预测4个位置偏移。。。。。为什么啊。



# NMS解释一下

**NMS（non maximum suppression）**，中文名非极大值抑制。





# 每个ROI维度不一样，怎么统一的来着





# anchor生成规则



# Focal Loss





