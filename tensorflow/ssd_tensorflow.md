## æ•°æ®é›†

### 1.1æ•°æ®é›†TFrecoedsåˆ¶ä½œ

PASCALVOC 2007ï¼Œç”¨äºç‰©ä½“æ£€æµ‹çš„æ³¨é‡Šåœ¨Annotationsæ–‡ä»¶å¤¹ä¸­ï¼Œæ¯å¼ å›¾ç‰‡å¯¹åº”ä¿¡æ¯ä¿å­˜ä¸ºxmlæ–‡ä»¶ã€‚

```xml
<annotation>
	<folder>VOC2007</folder>
	<filename>000001.jpg</filename>
	<source>  # æ²¡å•¥ç”¨
		<database>The VOC2007 Database</database>
		<annotation>PASCAL VOC2007</annotation>
		<image>flickr</image>
		<flickrid>341012865</flickrid>
	</source>
	<owner>  # æ²¡å•¥ç”¨
		<flickrid>Fried Camels</flickrid>
		<name>Jinky the Fruit Bat</name>
	</owner>
	<size>  # å›¾åƒå°ºå¯¸åŠé€šé“æ•°
		<width>353</width>
		<height>500</height>
		<depth>3</depth>
	</size>
	<segmented>0</segmented>
	<object>
		<name>dog</name>
		<pose>Left</pose>  # å¥½åƒæ˜¯ä»å“ªä¸ªè§’åº¦æ‹çš„ï¼Ÿ
		<truncated>1</truncated>  # è¿™ä¸ªç›®æ ‡æ˜¯å¦å› ä¸ºå„ç§åŸå› æ²¡æœ‰è¢«æ¡†å®Œæ•´ï¼ˆè¢«æˆªæ–­äº†ï¼‰
		<difficult>0</difficult>  # å¾…æ£€æµ‹ç›®æ ‡æ˜¯å¦å¾ˆéš¾è¯†åˆ«ï¼Œä¸º1çš„ç›®æ ‡åœ¨æµ‹è¯„ä¼°ä¸­ä¸€èˆ¬ä¼šè¢«å¿½ç•¥
		<bndbox>  # è½´å¯¹å…¶çŸ©å½¢ï¼Œæ¡†ä½çš„æ˜¯ç›®æ ‡åœ¨ç…§ç‰‡ä¸­çš„å¯è§éƒ¨åˆ†
			<xmin>48</xmin>
			<ymin>240</ymin>
			<xmax>195</xmax>
			<ymax>371</ymax>
		</bndbox>
	</object>
	<object>
		â€¦â€¦
	</object>
</annotation>

```

20ç±»åˆ«+1èƒŒæ™¯ä¿¡æ¯å¦‚ä¸‹ï¼Œä»¥å­—å…¸ç±»å‹å­˜å‚¨

```python
VOC_LABELS = {
    'none': (0, 'Background'),
    'aeroplane': (1, 'Vehicle'),
    'bicycle': (2, 'Vehicle'),
    'bird': (3, 'Animal'),
    'boat': (4, 'Vehicle'),
    'bottle': (5, 'Indoor'),
    'bus': (6, 'Vehicle'),
    'car': (7, 'Vehicle'),
    'cat': (8, 'Animal'),
    'chair': (9, 'Indoor'),
    'cow': (10, 'Animal'),
    'diningtable': (11, 'Indoor'),
    'dog': (12, 'Animal'),
    'horse': (13, 'Animal'),
    'motorbike': (14, 'Vehicle'),
    'person': (15, 'Person'),
    'pottedplant': (16, 'Indoor'),
    'sheep': (17, 'Animal'),
    'sofa': (18, 'Indoor'),
    'train': (19, 'Vehicle'),
    'tvmonitor': (20, 'Indoor'),
}
```

#### run

```python
DIRECTORY_ANNOTATIONS = 'Annotations/'
DIRECTORY_IMAGES = 'JPEGImages/'
# TFRecords convertion parameters.
RANDOM_SEED = 4242
SAMPLES_PER_FILES = 200  # æ¯ä¸ªtfrecorå­˜å‚¨çš„sampleæ•°é‡

def run(dataset_dir, output_dir, name='voc_train', shuffling=False):
    """Runs the conversion operation.
    Args:
      dataset_dir: The dataset directory. mine:F:/data/VOCdevkit/VOC2007
      output_dir: Output directory.æ¯”å¦‚ï¼š./tfrecords
      nameï¼šé»˜è®¤ä¸ºvoc_train,å¯¹åº”VOC2007ä¸ºvoc_2007_train
    """
    # True if the path exists, whether its a file or a directory
    if not tf.gfile.Exists(dataset_dir): 
        tf.gfile.MakeDirs(dataset_dir)
    # Dataset filenames, and shuffling.å¯¹äºæ£€æµ‹ä»»åŠ¡ï¼Œæ´—ä¸æ´—ç‰Œæ„Ÿè§‰æ²¡é‚£ä¹ˆé‡è¦
    path = os.path.join(dataset_dir, DIRECTORY_ANNOTATIONS)
    filenames = sorted(os.listdir(path))  # å¿…é¡»æ’åºï¼Œå› ä¸ºç³»ç»Ÿéƒ½å‡ºæ¥çš„å¯èƒ½ä¸æŒ‰ç…§é¡ºåº
    if shuffling:
        random.seed(RANDOM_SEED)  # ä¿è¯ä¸‹ä¸€å¥ï¼ˆæ´—ç‰Œï¼‰æ¯æ¬¡è¾“å‡ºä¸€æ ·çš„ç»“æœ
        random.shuffle(filenames)
    # Process dataset files.
    i = 0
    fidx = 0  # file_index,tfrecordsçš„åå­—ä¸­çš„ %03dï¼Œ9663=200*48+63ï¼Œä¼šç”Ÿæˆ49ä¸ªtfrecords
    while i < len(filenames):  # 9963ä¸ªå›¾ç‰‡
        # Open new TFRecord file.
        tf_filename = _get_output_filename(output_dir, name, fidx)  # å°±è·å¾—ä¸ªè·¯å¾„åå­—ã€‚ã€‚ã€‚
        with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:
            j = 0
            while i < len(filenames) and j < SAMPLES_PER_FILES:
                sys.stdout.write('\r>> Converting image %d/%d' % (i+1, len(filenames)))
                sys.stdout.flush()
                
                filename = filenames[i]
                img_name = filename[:-4]  # å»æ‰.jpg
                _add_to_tfrecord(dataset_dir, img_name, tfrecord_writer)
                i += 1
                j += 1
            fidx += 1
    print('\nFinished converting the Pascal VOC dataset!')
```

å…¶ä¸­ï¼š

```python
def _get_output_filename(output_dir, name, idx):
    return '%s/%s_%03d.tfrecord' % (output_dir, name, idx)
```

#####  _add_to_tfrecord()

1. å¤„ç†ä¸€å¼ å›¾ç‰‡åŠå…¶xmlæ–‡ä»¶ï¼Œè¯»å–ä¿¡æ¯
2. å°†ä¿¡æ¯è½¬æ¢ä¸ºexample
3. è¿ç”¨*tf.python_io.TFRecordWriter*å†™å…¥ä¸€å¼ å›¾ç‰‡ä¿¡æ¯ï¼ˆexampleï¼‰åˆ°æ–‡ä»¶ä¸­ï¼š

> çœŸä¸æ˜ç™½ä¸ºä»€ä¹ˆå•ç‹¬å†™ä¸€ä¸ªå‡½æ•°ã€‚ã€‚

```python
def _add_to_tfrecord(dataset_dir, name, tfrecord_writer):
    """Loads data from image and annotations files and add them to a TFRecord.

    Args:
      dataset_dir: Dataset directory;
      name: Image name to add to the TFRecord;
      tfrecord_writer: The TFRecord writer to use for writing.
    """
    image_data, shape, bboxes, labels, labels_text, difficult, truncated = \
        _process_image(dataset_dir, name)
    example = _convert_to_example(image_data, labels, labels_text,
                                  bboxes, shape, difficult, truncated)
    tfrecord_writer.write(example.SerializeToString())
```

###### _process_image

```python
def _process_image(directory, name):
    """Process a image and annotation file.
      dataset_dir: Dataset directory;
      name: Image name to add to the TFRecord;
    """
    # åŸæ¥æ˜¯ï¼šfilename = directory + DIRECTORY_IMAGES + name + '.jpg'
    # å¾ˆå¥‡æ€ªï¼Œdirectoryåé¢æ²¡æœ‰'/'å•Šï¼Œæ€ä¹ˆä¸ç”¨join
    # ä¸ºä»€ä¹ˆä¸ç›´æ¥ä¼ å…¥å¸¦åç¼€.jpgçš„æ–‡ä»¶å
    # å§æ§½ï¼Œæ‡‚äº†è¿˜æœ‰xmlæ–‡ä»¶å‘¢
    filename = directory +'/'+ DIRECTORY_IMAGES + name + '.jpg'
    # Read the image file.
    # åŸæ¥æ˜¯ï¼šimage_data = tf.gfile.FastGFile(filename, 'r').read()ï¼Œæœ‰bug
    # rbè¡¨ç¤ºæŒ‰å­—èŠ‚è¯»ï¼Œré»˜è®¤æ˜¯rtæ¨¡å¼ï¼šå­—èŠ‚æ–‡æœ¬ã€‚ä¸å¤ªæ‡‚
    image_data = tf.gfile.FastGFile(filename, 'rb').read()

    # Read the XML annotation file.è¿™åˆç”¨joinã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
    filename = os.path.join(directory, DIRECTORY_ANNOTATIONS, name + '.xml')
    tree = ET.parse(filename)  # è§£æ
    root = tree.getroot()

    # Image shape.
    size = root.find('size')
    shape = [int(size.find('height').text),
             int(size.find('width').text),
             int(size.find('depth').text)]  # æ¯”å¦‚1.jpg, [500,353,3]
    # Find annotations.æ¯ä¸ªobjectæœ‰ä»¥ä¸‹ä¿¡æ¯
    bboxes = []  # å½’ä¸€åŒ–å“¦ï¼Œå…ƒç´ ï¼štuple(ymin,xmin,ymax,xmax)
    labels = []
    labels_text = []
    difficult = []
    truncated = []
    for obj in root.findall('object'):
        label = obj.find('name').text
        labels.append(int(VOC_LABELS[label][0]))
        labels_text.append(label.encode('ascii'))

        if obj.find('difficult'):
            difficult.append(int(obj.find('difficult').text))
        else:
            difficult.append(0)
        if obj.find('truncated'):
            truncated.append(int(obj.find('truncated').text))
        else:
            truncated.append(0)

        bbox = obj.find('bndbox')
        bboxes.append((float(bbox.find('ymin').text) / shape[0],  # å½’ä¸€åŒ–äº†å•Š
                       float(bbox.find('xmin').text) / shape[1],
                       float(bbox.find('ymax').text) / shape[0],
                       float(bbox.find('xmax').text) / shape[1]
                       ))
    return image_data, shape, bboxes, labels, labels_text, difficult, truncated
```

å¾—åˆ°çš„ä¸»è¦çš„æœ‰ç”¨çš„å›¾ç‰‡ä¿¡æ¯ï¼š

```python
img_data:image_data = tf.gfile.FastGFile(filename, 'rb').read(),ä¸€å¤§ä¸²\xè¿›åˆ¶çš„
shape:[H,W,3]ï¼Œå½¢çŠ¶ä¸º(3,)
ä¸‹é¢å¯¹åº”å¤šä¸ªobjectï¼š
bboxes:[(ymin,xmin,ymax,xmax),(),()],å½¢çŠ¶ä¸º(num_object,)ï¼Œè®°ä½è¿™é‡Œå·²ç»å½’ä¸€åŒ–ï¼Œå¹¶ä¸”å°†åæ ‡æ”¾åœ¨äº†ä¸€èµ·ã€‚
labels:[c1,c2,c3â€¦â€¦],å½¢çŠ¶ä¸º(num_object,)
labels_text, difficult, truncatedå¥½åƒæ²¡å•¥ç”¨ã€‚
```

######  _convert_to_example

è¿”å›tf.train.Example(features=tf.train.Features(feature={â€¦â€¦}))å®ä¾‹ã€‚

```python
def _convert_to_example(image_data, labels, labels_text, bboxes, shape,
                        difficult, truncated):
    """Build an Example proto for an image example.

    Args:
      image_data: string, JPEG encoding of RGB image;
      labels: list of integers, identifier for the ground truth;
      labels_text: list of strings, human-readable labels;
      bboxes: list of bounding boxes; each box is a list of integers; # tupleå§ï¼Ÿï¼Ÿ
          specifying [xmin, ymin, xmax, ymax]. [ymin,xmin,ymax,xmax]å§ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
          All boxes are assumed to belong to the same label as the image label.
      shape: 3 integers, image shapes in pixels.
    Returns:
      Example proto
    """
    xmin = []
    ymin = []
    xmax = []
    ymax = []
    for b in bboxes:
        assert len(b) == 4
        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]
        
    image_format = b'JPEG'
    example = tf.train.Example(features=tf.train.Features(feature={
            'image/height': int64_feature(shape[0]),
            'image/width': int64_feature(shape[1]),
            'image/channels': int64_feature(shape[2]),
            'image/shape': int64_feature(shape),
            'image/object/bbox/xmin': float_feature(xmin),
            'image/object/bbox/xmax': float_feature(xmax),
            'image/object/bbox/ymin': float_feature(ymin),
            'image/object/bbox/ymax': float_feature(ymax),
            'image/object/bbox/label': int64_feature(labels),
            'image/object/bbox/label_text': bytes_feature(labels_text),
            'image/object/bbox/difficult': int64_feature(difficult),
            'image/object/bbox/truncated': int64_feature(truncated),
            'image/format': bytes_feature(image_format),
            'image/encoded': bytes_feature(image_data)}))
    return example

def int64_feature(value):
    """Wrapper for inserting int64 features into Example proto.
    """
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def float_feature(value):
    """Wrapper for inserting float features into Example proto.
    """
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(float_list=tf.train.FloatList(value=value))

def bytes_feature(value):
    """Wrapper for inserting bytes features into Example proto.
    """
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))
```

å…¶ä¸­ï¼š

```python
"""zip([iterable, ...])
å‚æ•°è¯´æ˜ï¼š
	iterabl -- ä¸€ä¸ªæˆ–å¤šä¸ªè¿­ä»£å™¨;è¿”å›zipå¯¹è±¡ï¼Œéœ€è¦ç”¨listè½¬æ¢å±•ç¤ºï¼Œæ‰“åŒ…ä¸ºtuple
"""
z = zip([ymin, xmin, ymax, xmax], (1,2,3,4))
print (list(z))  # [([], 1), ([], 2), ([], 3), ([], 4)]
```

#### å…³é”®æ­¥éª¤

```python
while i < len(filenames):
    tf_filename = ''
	with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:
    	image_data, shape, bboxes, labels = process(info(i)) # è‡ªå·±å»å¤„ç†æ•°æ®ç¬¬iä¸ªæ•°æ®ï¼Œæœ€å¥½list
         example = tf.train.Example(features=tf.train.Features(feature={
            'image/format': bytes_feature(image_format),
            'image/encoded': bytes_feature(image_data),
            'image/height': int64_feature(shape[0]),
            'image/width': int64_feature(shape[1]),
            'image/channels': int64_feature(shape[2]),
            'image/shape': int64_feature(shape),
            'image/object/bbox/xmin': float_feature(xmin),
            'image/object/bbox/xmax': float_feature(xmax),
            'image/object/bbox/ymin': float_feature(ymin),
            'image/object/bbox/ymax': float_feature(ymax),
            'image/object/bbox/label': int64_feature(labels)}))
    	tfrecord_writer.write(example.SerializeToString())
        i+=1
```

1. ç”ŸæˆTFRecord Writerï¼š

   ```python
   writer = tf.python_io.TFRecordWriter(path, options=None)
   ```

2. tf.train.Featuresç”Ÿæˆåè®®ä¿¡æ¯ï¼Œå†…å±‚featureæ˜¯å­—å…¸ï¼Œå­—å…¸keyç”¨äºè¯»å–æ—¶ç´¢å¼•ã€‚åˆ—è¡¨ç±»å‹ä¸€èˆ¬æœ‰BytesList, FloatList, Int64Listï¼Œ ä¾‹å¦‚ï¼š

   ```python
   feature = {
   "width":tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),
   "weights":tf.train.Feature(float_list=tf.train.FloatList(value=[weights])),
   "image_raw":tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_raw]))
   }
   ```
   å…¶ä¸­

   ```python
   tf.train.BytesList(value=[value]) # valueè½¬åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ˆäºŒè¿›åˆ¶ï¼‰åˆ—è¡¨
   tf.train.FloatList(value=[value]) # valueè½¬åŒ–ä¸ºæµ®ç‚¹å‹åˆ—è¡¨
   tf.train.Int64List(value=[value]) # valueè½¬åŒ–ä¸ºæ•´å‹åˆ—è¡¨
   ```

   å¤–å±‚featureså†å°†å†…å±‚å­—å…¸ç¼–ç ï¼š 

   ```python
   features = tf.train.Features(feature)
   ```

3. ä½¿ç”¨tf.train.Exampleå°†featuresç¼–ç æ•°æ®å°è£…æˆç‰¹å®šçš„PBåè®®æ ¼å¼

   ```python
   example = tf.train.Example(features)
   ```
   printçš„æ•ˆæœï¼š

   ```
   features {
     feature {
       key: "image/data"
       value {
         bytes_list {
           value: "\224\224..."
         }
       }
     }
     feature {
       key: "image/label"
       value {
         int64_list {
           value: 0
         }
       }
     }
   }        
   ```

   

4. å°†exampleæ•°æ®ç³»åˆ—åŒ–ä¸ºå­—ç¬¦ä¸²

   ```python
   example_str = example.SerializeToString()
   ```

5. å°†ç³»åˆ—åŒ–ä¸ºå­—ç¬¦ä¸²çš„exampleæ•°æ®å†™å…¥åè®®ç¼“å†²åŒº

   ```python
   writer.write(example_str)
   writer.close()
   ```


### 1.2æ•°æ®é›†è¯»å–

#### tf.TFRecordReader

```python
    fliename = r'./voc_2007_train_000.tfrecord'
    # ä¸€ã€æ ¹æ®æ–‡ä»¶åç”Ÿæˆæ–‡ä»¶åé˜Ÿåˆ—
    filename_queue = tf.train.string_input_producer([filename])  # tfrecordæ–‡ä»¶ålist
    reader = tf.TFRecordReader()  # äºŒã€ç”ŸæˆTFRecordReader
    tf_name, serialized_example = reader.read(filename_queue)  # è¿”å›æ–‡ä»¶åå’Œè¯»å–çš„å†…å®¹
    # ä¸‰ã€è§£æå™¨è§£æåºåˆ—åŒ–example
    features = tf.parse_single_example(serialized_example,
                features={
                # å¯¹äºå•ä¸ªå…ƒç´ çš„å˜é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨FixlenFeatureæ¥è¯»å–ï¼Œéœ€è¦æŒ‡æ˜å˜é‡å­˜å‚¨çš„æ•°æ®ç±»å‹
                'image/encoded': tf.FixedLenFeature([], tf.string,default_value=''),
                'image/format': tf.FixedLenFeature([], tf.string,default_value='jpeg'),
                'image/height': tf.FixedLenFeature([1], tf.int64),
                'image/width': tf.FixedLenFeature([1], tf.int64),
                'image/channels': tf.FixedLenFeature([1], tf.int64),
                'image/shape': tf.FixedLenFeature([3], tf.int64),
                # å¯¹äºlistç±»å‹çš„å˜é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨VarLenFeatureæ¥è¯»å–ï¼ŒåŒæ ·éœ€è¦æŒ‡æ˜è¯»å–å˜é‡çš„ç±»å‹
                # bbox
                'image/object/bbox/xmin':tf.VarLenFeature(dtype=tf.float32),
                'image/object/bbox/ymin':tf.VarLenFeature(dtype=tf.float32),
                'image/object/bbox/xmax':tf.VarLenFeature(dtype=tf.float32),
                'image/object/bbox/ymax':tf.VarLenFeature(dtype=tf.float32),
                # label
                'image/object/bbox/label':tf.VarLenFeature(dtype=tf.int64),
                })
    
    # featureså­—å…¸ï¼Œæ¥ä¸‹æ¥ç”¨keyç´¢å¼•å¾—åˆ°æ•°æ®ï¼Œä¸ºæˆ‘æ‰€ç”¨ã€‚
```

ä¾‹å¦‚ï¼Œæ˜¾ç¤º000001.jpgå›¾ç‰‡ï¼š

```python
print(features) # 1æ ‡æ³¨
import matplotlib.pyplot as plt
with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)
    shape,ymin,img = \
sess.run([features['image/shape'],features['image/object/bbox/ymin'],features['image/encoded']])
    print(shape)  # [500 353   3]
    print(ymin)  # 2æ ‡æ³¨

    image = tf.image.decode_jpeg(img, channels=3) #Tensor("DecodeJpeg:0", shape=(?, ?, 3), dtype=uint8) æœ€å¥½ä¸è¦åœ¨sessioné‡Œé¢å†™Opçš„æ“ä½œã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
    image = sess.run(image)  # typeï¼šnumpy.ndarray
    print(image.dtype)  # uint8

    plt.imshow(image)
    plt.show()

    coord.request_stop()
    coord.join(threads)
```

æ³¨æ„ï¼š#1æ ‡æ³¨ï¼Œ#2æ ‡æ³¨çš„æ‰“å°ç»“æœï¼š

```python
#1 features:æ˜¯å­—å…¸
{'image/object/bbox/label': 
<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000001DBA49CDD68>,
'image/object/bbox/ymax':
<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000001DBA4A06D68>,
 'image/encoded': 
 <tf.Tensor 'ParseSingleExample/ParseSingleExample:16' shape=() dtype=string>,
 'image/height': 
 <tf.Tensor 'ParseSingleExample/ParseSingleExample:18' shape=(1,) dtype=int64>,
 }
```

+ ä»¥listå­˜å‚¨çš„labelå’Œymaxç­‰åæ ‡å€¼å–å‡ºæ¥çš„ç±»å‹æ˜¯**SparseTensor**ï¼›
+ è€Œimageæ˜¯**string**ç±»å‹ï¼Œåº”è¯¥æ˜¯å­—èŠ‚å‹å­—ç¬¦ä¸²ï¼Œåƒè¿™æ ·ï¼š

```
b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00\x00â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦'
```

+ é«˜ã€å®½ã€é€šé“æ•°ã€å’Œå½¢çŠ¶éƒ½æ˜¯int64ç±»å‹çš„ï¼Œæƒ…ç†ä¹‹ä¸­ã€‚

```python
#2 è€Œyminçš„ç±»å‹æ˜¯ï¼štf.VarLenFeature(dtype=tf.float32),å–å‡ºæ¥æ˜¯ç¨€ç–Tensor..........
SparseTensorValue(indices=array([[0],[1]], dtype=int64), 
                  values=array([0.48 , 0.024], dtype=float32), 
                  dense_shape=array([2], dtype=int64))
# æ€ä¹ˆå°±æ˜¯ç¨€ç–çŸ©é˜µäº†å‘¢ï¼Ÿï¼Ÿï¼Ÿå¦‚ä½•è½¬æ¢æˆå¯ä»¥æ­£å¸¸çš„array([0.48 , 0.024], dtype=float32)å‘¢ï¼Ÿï¼Ÿé 
```

#### è¯»å–å¤šå¼ 

åœ¨æ„å»ºå¥½å›¾ä¹‹åï¼Œrunï¼ŒimageèŠ‚ç‚¹å¤šæ¬¡å°±å¯ä»¥ä¾æ¬¡å–å‡ºå›¾åƒæ•°æ®ã€‚

```python
filename_queue = tf.train.string_input_producer([tfrecord_path])
reader = tf.TFRecordReader()
# ç”¨readerå»readæ•°æ®é›†é˜Ÿåˆ—
tf_name, serialized_example = reader.read(filename_queue,name='reading')
features = tf.parse_single_example(serialized_example,
    features={
              'image/encoded': tf.FixedLenFeature([], tf.string, default_value=''),
    })
img = features['image/encoded']
image = tf.image.decode_jpeg(img, channels=3)
=======================æ„å»ºå›¾å®Œæˆ=========================
with tf.Session() as sess:
writer = tf.summary.FileWriter('./graphs',sess.graph)
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)

print(image)
img1 = sess.run(image)
img2 = sess.run(image)  # runäº†ä¸¤æ¬¡imageç»“ç‚¹å°±å¯ä»¥ä¾æ¬¡å–å‡ºå›¾åƒ
plt.imshow(img1)
plt.show()
plt.imshow(img2)
plt.show()
coord.request_stop()
coord.join(threads)
```

![1533044907809](assets/1533044907809.png)
![1533044917245](assets/1533044917245.png)


#### tf.python_io.tf_record_iterator 

```python
with tf.Session() as sess:
    for serialized_example in tf.python_io.tf_record_iterator(tfrecord_path):
        # An iterator that read the records from a TFRecords file
        features = tf.parse_single_example(serialized_example,
            features={
            'image/encoded': tf.FixedLenFeature([], tf.string, default_value=''),
            'image/height': tf.FixedLenFeature([1], tf.int64),
            'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),
            'image/object/bbox/label': tf.VarLenFeature(dtype=tf.int64),
            })
        height = features['image/height']
        print(sess.run(height))
```

### 1.3slimåº“çš„æ•°æ®é›†è¯»å–

#### slim.dataset.Dataset

è¯´ç™½äº†ï¼Œè¿™ä¸ªç±»å°±æ˜¯ä¸ªç±»ä¼¼äº`nametuple`çš„ç±»ï¼Œä»–è‡ªå·±æ˜¯æ²¡ä»€ä¹ˆåµç”¨ã€‚

> More concretely, TF-Slim's [dataset](https://tensorflow.google.cn/code/tensorflow/contrib/slim/python/slim/data/dataset.py) 
> is **a tuple** that encapsulates the following elements of a dataset specification:
>
> + `data_sources`: A list of file paths that together make up the dataset
> + `reader`: A TensorFlow [Reader](https://tensorflow.google.cn/api_docs/python/io_ops.html#ReaderBase) appropriate for the file type in `data_sources`.
> + `decoder`: A TF-Slim [data_decoder](https://tensorflow.google.cn/code/tensorflow/contrib/slim/python/slim/data/data_decoder.py) class which is used to decode the content of the read dataset files.
> + `num_samples`: The number of samples in the dataset.
> + `items_to_descriptions`: A map from the items provided by the dataset to descriptions of each.
>
> In a nutshell, a dataset is read by (a) opening the files specified by `data_sources` using the given `reader` class (b) decoding the files using the given `decoder` and (c) allowing the user to request a list of `items` to be returned as `Tensors`.

```python
class Dataset(object):
  """Represents a Dataset specification."""

  def __init__(self, data_sources, reader, decoder, num_samples,
               items_to_descriptions, **kwargs):
        
    print(kwargs)
    kwargs['data_sources'] = data_sources # ./tfrecords/voc2007_train_*.tfrecord
    kwargs['reader'] = reader
    kwargs['decoder'] = decoder
    kwargs['num_samples'] = num_samples
    kwargs['items_to_descriptions'] = items_to_descriptions
    print(kwargs)
    self.__dict__.update(kwargs)
    print(self.__dict__)
```

ä¸Šé¢ä¸‰è¡Œ`print`æ˜¯æ‰‹åŠ¨åŠ çš„ï¼Œçœ‹ä¸€ä¸‹å®ä¾‹åŒ–ä¸€ä¸ªå¯¹è±¡çš„æ‰“å°è¾“å‡ºï¼š

```python
dataset = Dataset(
            data_sources='./tfrecords/voc2007_train_*.tfrecord',
            reader=tf.TFRecordReader,  # reader = tf.TFRecordReader
            decoder='decoder',
            num_samples= 200,
            items_to_descriptions='hahahhaha',
            num_classes=21,
            others='i dont know')
# ä»…ä¸ºäº†æ¼”ç¤ºï¼Œå‚æ•°éƒ½ä¼ å…¥å­—ç¬¦ä¸²äº†,è¿™é‡Œå…³é”®å­—å‚æ•°æœ‰2ä¸ªã€‚
>>>
{'num_classes': 21, 'others': 'i dont know'}
{'num_classes': 21, 'others': 'i dont know', 
 'data_sources': './tfrecords/voc2007_train_*.tfrecord', 
 'reader': <class 'tensorflow.python.ops.io_ops.TFRecordReader'>, 'decoder': 'decoder', 
 'num_samples': 200, 'items_to_descriptions': 'haha'}
# æœ€åçš„kwargsä¸__dict__çš„å€¼ä¸€æ ·ï¼Œéƒ½æ˜¯å±æ€§åŠå€¼çš„å­—å…¸ã€‚
```

å…³é”®å­—å‚æ•°ï¼Œ\*\*kwargsåœ¨ä¼ å…¥æ—¶ä¼šæ‰“åŒ…æˆå­—å…¸dictï¼Œå¯ä»¥çœ‹åˆ°è¿™é‡Œå…è®¸ç”¨æˆ·ä¼ å…¥ä¸€äº›å…¶ä»–ä¸æ•°æ®é›†ç›¸å…³çš„å‚æ•°ï¼Œè¿™é‡Œä¸»è¦ç†è§£ï¼š`__dict__`æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œit contains all the attributes which describe the object ã€‚çŒœæµ‹å­—å…¸çš„`updata`æ–¹æ³•å¯ä»¥æ›´æ–°å±æ€§å€¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ–°å¢ã€‚

##### Data Decodersï¼šTFExampleDecoder

æ„é€ slim.dataset.Datasetæœ€é‡è¦çš„åº•ä¸‹è¿™ä¸¤è¡Œäº†ã€‚

```python
reader=tf.TFRecordReader,
decoder=ï¼Ÿ,
```

readerå’Œæ•°æ®é›†æ ¼å¼æœ‰å…³ï¼Œdecoderä¸readå‡ºæ¥çš„æ•°æ®æ ¼å¼æœ‰å…³ã€‚ä¸‹é¢çš„ä»£ç æ˜¯TFExampleDecoderçš„ç”Ÿæˆä»£ç ï¼Œå¯ä»¥çœ‹åˆ°å‚æ•°`keys_to_features`æ ¼å¼ä¸ç”¨[tf.TFRecordReader](#tf.tfrecordreader)è¯»å–æ•°æ®æ—¶æ‰€ç”¨çš„æ—¶ç›¸åŒçš„å­—å…¸æ ¼å¼ï¼š

> AÂ `TFExample`Â protocol buffer is a map from keys (strings) to either aÂ  `tf.FixedLenFeature`Â  or `tf.VarLenFeature`.  
>
> Consequently, to decode a`TFExample`, one must provide a mapping from one or moreÂ `TFExample`Â fields to each of theÂ `items`Â that theÂ `tfexample_decoder`Â can provide. 
>

`TFExample`å­—æ®µåº”è¯¥æŒ‡çš„æ˜¯`features`å­—å…¸é‡Œçš„å„ä¸ªé”®å€¼å§ã€‚é€šè¿‡ä¸‹é¢çš„ä»£ç åŠä¸Šé¢çš„å¼•ç”¨å¯ä»¥çœ‹åˆ°ï¼Œ`TFExampleDecoder`å¯ä»¥æä¾›çš„itemså¹¶ä¸æ˜¯ä¸featuresçš„ä¸€ä¸€å¯¹åº”ï¼Œ**è€Œæ˜¯ä¸€å¯¹ä¸€ï¼Œæˆ–è€…å¤šå¯¹ä¸€**ã€‚

```python
keys_to_features = {
    # å¯¹äºå•ä¸ªå…ƒç´ çš„å˜é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨FixlenFeatureæ¥è¯»å–ï¼Œéœ€è¦æŒ‡æ˜å˜é‡å­˜å‚¨çš„æ•°æ®ç±»å‹
    'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
    'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),
    # shape
    'image/height': tf.FixedLenFeature([1], tf.int64),
    'image/width': tf.FixedLenFeature([1], tf.int64),
    'image/channels': tf.FixedLenFeature([1], tf.int64),
    'image/shape': tf.FixedLenFeature([3], tf.int64),
    # å¯¹äºlistç±»å‹çš„å˜é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨VarLenFeatureæ¥è¯»å–ï¼ŒåŒæ ·éœ€è¦æŒ‡æ˜è¯»å–å˜é‡çš„ç±»å‹
    # bbox
    'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),
    'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),
    'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),
    'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32),
    # label
    'image/object/bbox/label': tf.VarLenFeature(dtype=tf.int64),
    'image/object/bbox/difficult': tf.VarLenFeature(dtype=tf.int64),
    'image/object/bbox/truncated': tf.VarLenFeature(dtype=tf.int64),
}
items_to_handlers = {
    'image': slim.tfexample_decoder.Image('image/encoded', 'image/format'),
    'shape': slim.tfexample_decoder.Tensor('image/shape'),
    'object/bbox': slim.tfexample_decoder.BoundingBox(
            ['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/'),
    'object/label': slim.tfexample_decoder.Tensor('image/object/bbox/label'),
    'object/difficult': slim.tfexample_decoder.Tensor('image/object/bbox/difficult'),
    'object/truncated': slim.tfexample_decoder.Tensor('image/object/bbox/truncated'),
}
decoder = slim.tfexample_decoder.TFExampleDecoder(
    keys_to_features, items_to_handlers)
```

ä¸Šè¿°æ‰€ç”¨åˆ°çš„å®ä¾‹éƒ½åœ¨`slim.tfexample_decoder.py`æ–‡ä»¶ä¸­å®šä¹‰ï¼Œæˆ‘ä»¬å»çœ‹ä¸€çœ‹[tfexample_decoder.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py)ï¼š

+ `keys_to_features`[è¿™ä¸ª](#Data Decodersï¼šTFExampleDecoder)ä¸Šé¢è§£é‡Šäº†ï¼ŒTFExample keyï¼š`tf.VarLenFeature` or `tf.FixedLenFeature`instances
+ `items_to_handlers`ä¹Ÿæ˜¯å­—å…¸ï¼Œa dictionary from items (strings) to `ItemHandler`  instancesï¼Œè‡ªå·±èµ·çš„å­—ç¬¦ä¸²ä¸ItemHandlerå®ä¾‹çš„æ˜ å°„ã€‚

ä¸Šé¢ä¸¤ä¸ªå‚æ•°æ„æˆäº†`slim.tfexample_decoder.TFExampleDecoder`ã€‚

è€Œé‚£äº› `ItemHandler` å®ä¾‹éƒ½æ˜¯ç»§æ‰¿è‡ª `Class ItemHandler` ï¼š

```python
class ItemHandler(object):
  """Specifies the item-to-Features mapping for tf.parse_example.
  æ—¢æŒ‡å®šç”¨äºè§£æExample protoçš„éƒ¨åˆ†Featuresçš„åˆ—è¡¨ï¼Œå³ç”¨äºå¤šå¯¹ä¸€çš„å¤šã€‚
  ä¹ŸæŒ‡å®šç”¨äºå¯¹Exampleè§£æçš„ç»“æœè¿›è¡Œåå¤„ç†çš„å‡½æ•°ï¼Œå³è¿”å›å¤šå¯¹ä¸€çš„ä¸€ã€‚
  """
  __metaclass__ = abc.ABCMeta

  def __init__(self, keys):
    """Constructs the handler with the name of the tf.Feature keys to use.
    Args:
      keys: the name of the TensorFlow Example Feature.
    """
    if not isinstance(keys, (tuple, list)):
      keys = [keys]
    self._keys = keys  # ä¿å­˜æ‰€æœ‰ç”¨åˆ°çš„Feature name

  @property
  def keys(self):
    return self._keys

  @abc.abstractmethod
  def tensors_to_item(self, keys_to_tensors):
    """Maps the given dictionary of tensors to the requested item.

    Args:
      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.

    Returns:
      the final tensor representing the item being handled.
    """
    pass
```

é‚£Imageã€Tensorã€BoundingBoxè¿™äº›å­ç±» `ItemHandler` éƒ½å¹²äº†ä»€ä¹ˆå‘¢ï¼Ÿæœ‰è®¸å¤šå­ç±»ï¼Œç›®å‰æˆ‘ä»¬åªçœ‹è¿™ä¸‰ä¸ªï¼š

+ `class Tensor(ItemHandler)`

```python
class Tensor(ItemHandler):
  """An ItemHandler that returns a parsed Tensor."""

  def __init__(self, tensor_key, shape_keys=None, shape=None, default_value=0):
    """Initializes the Tensor handler.

    tensor_keyï¼šthe name of the `TFExample` feature to read the tensor from.
    è¿”å›no reshapingçš„Tensorï¼Œä½†ä»ç„¶å¯ä»¥æŒ‡å®šå½¢çŠ¶ï¼Œæ¥æºæœ‰ä¸¤ä¸ªï¼šshape_keyï¼Œè¿™ä¸ªä¹Ÿæ˜¯TFExample featureçš„åå­—æˆ–åå­—åˆ—è¡¨ï¼Œshapeåˆ™æ˜¯äººä¸ºæä¾›ã€‚
    """
    if shape_keys and shape is not None:
      raise ValueError('Cannot specify both shape_keys and shape parameters.')
    if shape_keys and not isinstance(shape_keys, list):
      shape_keys = [shape_keys]
    self._tensor_key = tensor_key
    self._shape_keys = shape_keys
    self._shape = shape
    self._default_value = default_value
    keys = [tensor_key]
    if shape_keys:
      keys.extend(shape_keys)  # æŠŠæ‰€æœ‰keysç”¨æ¥åˆå§‹åŒ–_key
    super(Tensor, self).__init__(keys)

  def tensors_to_item(self, keys_to_tensors): 
     # keys_to_tensorsä¸çŸ¥é“æ˜¯å•¥ã€‚ã€‚ã€‚åº”è¯¥å°±æ˜¯tfexampleè§£æå‡ºæ¥çš„å­—å…¸
     # keys_to_tensors: a mapping of TF-Example keys to parsed tensors.
    tensor = keys_to_tensors[self._tensor_key]
    shape = self._shape
    if self._shape_keys:
      shape_dims = []
      for k in self._shape_keys:
        shape_dim = keys_to_tensors[k]
        if isinstance(shape_dim, sparse_tensor.SparseTensor):
          shape_dim = sparse_ops.sparse_tensor_to_dense(shape_dim)
        shape_dims.append(shape_dim)
      shape = array_ops.reshape(array_ops.stack(shape_dims), [-1])
    if isinstance(tensor, sparse_tensor.SparseTensor):
      if shape is not None:
        tensor = sparse_ops.sparse_reshape(tensor, shape)
      tensor = sparse_ops.sparse_tensor_to_dense(tensor, self._default_value)
    else:
      if shape is not None:
        tensor = array_ops.reshape(tensor, shape)
    return tensor
```

+ `class Image(ItemHandler)`

```python
class Image(ItemHandler):
  """An ItemHandler that decodes a parsed Tensor as an image."""

  def __init__(self,
               image_key=None,
               format_key=None,
               shape=None,
               channels=3,
               dtype=dtypes.uint8,
               repeated=False,
               dct_method=''):
    """Initializes the image.

    Args:
      dtype: images will be decoded at this bit depth. Different formats
        support different bit depths.
          See tf.image.decode_image,
              tf.decode_raw,
      repeated: if False, decodes a single image. If True, decodes a
        variable number of image strings from a 1D tensor of strings.
      dct_method: An optional string. Defaults to empty string. It only takes
        effect when image format is jpeg, used to specify a hint about the
        algorithm used for jpeg decompression. Currently valid values
        are ['INTEGER_FAST', 'INTEGER_ACCURATE']. The hint may be ignored, for
        example, the jpeg library does not have that specific option.
    """
    if not image_key:
      image_key = 'image/encoded'
    if not format_key:
      format_key = 'image/format'
    # æŠŠç»™å‡ºçš„ä¸ç®¡å‡ ä¸ªfeatures keyä»˜ç»™ç»§æ‰¿çˆ¶ç±»çš„_key
    super(Image, self).__init__([image_key, format_key])  
    self._image_key = image_key
    self._format_key = format_key
    self._shape = shape
    self._channels = channels
    self._dtype = dtype
    self._repeated = repeated
    self._dct_method = dct_method

  def tensors_to_item(self, keys_to_tensors):
    """See base class."""
    image_buffer = keys_to_tensors[self._image_key]
    image_format = keys_to_tensors[self._format_key]

    if self._repeated:
      return functional_ops.map_fn(lambda x: self._decode(x, image_format),
                                   image_buffer, dtype=self._dtype)
    else:
      return self._decode(image_buffer, image_format)

  def _decode(self, image_buffer, image_format):
        # å‡½æ•°å°±ä¸å±•ç¤ºäº†
        return image
```

+ `class BoundingBox(ItemHandler)`ï¼šåœ¨æ£€æµ‹ä»»åŠ¡ä¸­ç»å¸¸ä¼šç”¨åˆ°å“¦ã€‚

  å‚æ•°ä¸€èˆ¬ä¸ºkeys = ['ymin', 'xmin', 'ymax', 'xmax']ï¼Œå…¶ä¸­éƒ½æ˜¯ä¸€å¼ å›¾åƒä¸Šå¤šä¸ªbboxçš„åæ ‡listã€‚

```python
class BoundingBox(ItemHandler):
  """An ItemHandler that concatenates a set of parsed Tensors to Bounding Boxes.
  """

  def __init__(self, keys=None, prefix=None):
    """Initialize the bounding box handler.

    Args:
      keys: A list of four key names representing the ymin, xmin, ymax, mmax
      prefix: An optional prefix for each of the bounding box keys.
        If provided, `prefix` is appended to each key in `keys`.

    Raises:
      ValueError: if keys is not `None` and also not a list of exactly 4 keys
    """
    if keys is None:
      keys = ['ymin', 'xmin', 'ymax', 'xmax']
    elif len(keys) != 4:
      raise ValueError('BoundingBox expects 4 keys but got {}'.format(
          len(keys)))
    self._prefix = prefix
    self._keys = keys
    self._full_keys = [prefix + k for k in keys]
    super(BoundingBox, self).__init__(self._full_keys)

  def tensors_to_item(self, keys_to_tensors):
    """Maps the given dictionary of tensors to a concatenated list of bboxes.

    Args:
      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.

    Returns:
      [num_boxes, 4] tensor of bounding box coordinates,
        i.e. 1 bounding box per row, in order [y_min, x_min, y_max, x_max].
    """
    sides = []
    for key in self._full_keys:
      side = keys_to_tensors[key]
      if isinstance(side, sparse_tensor.SparseTensor):
        side = side.values
      side = array_ops.expand_dims(side, 0)  # ä¾‹å¦‚[4,5] -> [[4,5]] (2)->(1,2)
      sides.append(side)

    bounding_box = array_ops.concat(sides, 0)  # (4,2)
    # ä¸Šé¢ä¸¤æ­¥æ“ä½œæ˜¯å…ˆæ‰©ç»´ï¼Œç„¶åè¿æ¥ï¼›ä¹Ÿå¯ä»¥ç›´æ¥stackå§
    return array_ops.transpose(bounding_box)  # shape = (2,4)
```

+ `class TFExampleDecoder(data_decoder.DataDecoder)`

```python
class TFExampleDecoder(data_decoder.DataDecoder):
  """A decoder for TensorFlow Examples.
è§£ç Example proto buffersåˆ†ä¸¤æ­¥ï¼Œ(1)è§£æExampleï¼Œå¾—åˆ°a set of tensorsï¼Œ(2)å¤„ç†ç¬¬ä¸€æ­¥å¾—åˆ°çš„tensorså¾—åˆ°ç”¨æˆ·éœ€è¦çš„'item' tensorsï¼Œä¹Ÿå°±æ˜¯å¤šå¯¹ä¸€çš„è¿‡ç¨‹ã€‚
æ‰€ä»¥ï¼Œå¯¹äºç¬¬ä¸€æ­¥è§£æéœ€è¦keys_to_featuresï¼Œç¬¬äºŒæ­¥éœ€è¦a list of ItemHandlersï¼Œæ¥å‘Šè¯‰Decoderå¦‚ä½•post_processing ç¬¬ä¸€éƒ¨å¾—åˆ°çš„tensors.
  """

  def __init__(self, keys_to_features, items_to_handlers):
    """Constructs the decoder."""
    self._keys_to_features = keys_to_features
    self._items_to_handlers = items_to_handlers

  def list_items(self):
    """See base class."""
    return list(self._items_to_handlers.keys())

  def decode(self, serialized_example, items=None):
    """Decodes the given serialized TF-example.

    Args:
      serialized_example: a serialized TF-example tensor.
      items: the list of items to decode. These must be a subset of the item
        keys in self._items_to_handlers. If `items` is left as None, then all
        of the items in self._items_to_handlers are decoded.

    Returns:
      the decoded items, a list of tensor.
    """
    example = parsing_ops.parse_single_example(serialized_example,
                                               self._keys_to_features)

    # Reshape non-sparse elements just once, adding the reshape ops in
    # deterministic order.
    for k in sorted(self._keys_to_features):
      v = self._keys_to_features[k]
      if isinstance(v, parsing_ops.FixedLenFeature):
        example[k] = array_ops.reshape(example[k], v.shape)

    if not items:
      items = self._items_to_handlers.keys()

    outputs = []
    for item in items:
      handler = self._items_to_handlers[item]
      keys_to_tensors = {key: example[key] for key in handler.keys}
      # æˆ‘å°±æƒ³çŸ¥é“keys_to_tensorsè¿™ä¸ªåˆ°åº•å’‹æ¥çš„ï¼Ÿç­”æ¡ˆâ†‘ğŸ‘†ï¼šå…¶å®å°±æ˜¯è§£æåexampleçš„å­é›†
      outputs.append(handler.tensors_to_item(keys_to_tensors))
    return outputs
```

ç»§æ‰¿`DataDecoder`æŠ½è±¡ç±»ï¼š

```python
import abc  # è¿™æè«çš„ä»€ä¹ˆé¬¼ï¼Ÿï¼Ÿï¼Ÿ
class DataDecoder(object):
  """An abstract class which is used to decode data for a provider."""

  __metaclass__ = abc.ABCMeta

  @abc.abstractmethod
  def decode(self, data, items):
    """
    Args:
      data: A possibly encoded data format.
      items: A list of strings, each of which indicate a particular data type.
    Returns:
      A list of `Tensors`, whose length matches the length of `items`, where
      each `Tensor` corresponds to each item.
    """
    pass

  @abc.abstractmethod
  def list_items(self):
    """Lists the names of the items that the decoder can decode.
    Returns:
      A list of string names.
    """
    pass
```

#### DatasetDataProvider

å®šä¹‰åœ¨[`slim.dataset_data_provider.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py)ï¼Œä¸è§£é‡Šæºç äº†ï¼Œä¼šç”¨å°±å¾—äº†ã€‚

ä½¿ç”¨ä¾‹å­ï¼š

```python
provider = slim.dataset_data_provider.DatasetDataProvider(
    dataset,
    num_readers=FLAGS.num_readers,
    common_queue_capacity=20 * FLAGS.batch_size,
    common_queue_min=10 * FLAGS.batch_size,
    shuffle=True)
```

```python
class DatasetDataProvider(data_provider.DataProvider):

  def __init__(self,
               dataset,
               num_readers=1,
               reader_kwargs=None,
               shuffle=True,
               num_epochs=None,
               common_queue_capacity=256,
               common_queue_min=128,
               record_key='record_key',
               seed=None,
               scope=None):
    """Creates a DatasetDataProvider.
    Note: if `num_epochs` is not `None`,  local counter `epochs` will be created
    by relevant function. Use `local_variables_initializer()` to initialize
    local variables.
    Args:
      dataset: An instance of the Dataset class.
      num_readers: The number of parallel readers to use.
      reader_kwargs: An optional dict of kwargs for the reader.
      shuffle: Whether to shuffle the data sources and common queue when
        reading.
      num_epochs: The number of times each data source is read. If left as None,
        the data will be cycled through indefinitely.
      common_queue_capacity: The capacity of the common queue.
      common_queue_min: The minimum number of elements in the common queue after
        a dequeue.
      record_key: The item name to use for the dataset record keys in the
        provided tensors.
      seed: The seed to use if shuffling.
      scope: Optional name scope for the ops.
    Raises:
      ValueError: If `record_key` matches one of the items in the dataset.
    """
    key, data = parallel_reader.parallel_read(
        # ./tfrecords/voc2007_train_*.tfrecordï¼ŒåŒ¹é…*åº”è¯¥æ˜¯åœ¨è¿™ä¸ªå‡½æ•°å®Œæˆï¼Œæœ‰æ—¶é—´å†çœ‹å§ã€‚
        dataset.data_sources,
        reader_class=dataset.reader,
        num_epochs=num_epochs,
        num_readers=num_readers,
        reader_kwargs=reader_kwargs,
        shuffle=shuffle,
        capacity=common_queue_capacity,
        min_after_dequeue=common_queue_min,
        seed=seed,
        scope=scope)

    items = dataset.decoder.list_items()
    tensors = dataset.decoder.decode(data, items)

    items_to_tensors = dict(zip(items, tensors))
    if record_key in items_to_tensors:
      raise ValueError('The item name used for `record_key` cannot also be '
                       'used for a dataset item: %s', record_key)
    items_to_tensors[record_key] = key

    super(DatasetDataProvider, self).__init__(
        items_to_tensors=items_to_tensors,
        num_samples=dataset.num_samples)
```

ç»§æ‰¿ï¼šæœ€ä¸»è¦çš„æ–¹æ³•å°±æ˜¯`get(items)`äº†

```python
class DataProvider(object):
  """Maps a list of requested data items to tensors from a data source.

  All data providers must inherit from DataProvider and implement the Get
  method which returns arbitrary types of data. No assumption is made about the
  source of the data nor the mechanism for providing it.
  """
  __metaclass__ = abc.ABCMeta

  def __init__(self, items_to_tensors, num_samples):
    """Constructs the Data Provider.

    Args:
      items_to_tensors: a dictionary of names to tensors.
      num_samples: the number of samples in the dataset being provided.
    """
    self._items_to_tensors = items_to_tensors
    self._num_samples = num_samples

  def get(self, items):
    """Returns a list of tensors specified by the given list of items.

    The list of items is arbitrary different data providers satisfy different
    lists of items. For example the Pascal VOC might accept items 'image' and
    'semantics', whereas the NYUDepthV2 data provider might accept items
    'image', 'depths' and 'normals'.

    Args:
      items: a list of strings, each of which indicate a particular data type.

    Returns:
      a list of tensors, whose length matches the length of `items`, where each
      tensor corresponds to each item.

    Raises:
      ValueError: if any of the items cannot be satisfied.
    """
    self._validate_items(items)
    return [self._items_to_tensors[item] for item in items]

  def list_items(self):
    """Returns the list of item names that can be provided by the data provider.

    Returns:
      a list of item names that can be passed to Get([items]).
    """
    return self._items_to_tensors.keys()

  def num_samples(self):
    """Returns the number of data samples in the dataset.

    Returns:
      a positive whole number.
    """
    return self._num_samples
```

### 1.4æ€»ç»“æµç¨‹

è‡³æ­¤æˆ‘ä»¬é€šè¿‡`provider.get()`å¾—åˆ°äº†æˆ‘ä»¬æƒ³è¦çš„æ•°æ®ï¼Œç”±äºä¹‹å‰å¯¹ä»£ç çš„ä¸ç†Ÿæ‚‰ï¼Œè‡ªåº•å‘ä¸Šæ¢³ç†äº†ä¸€éä»£ç ï¼Œç°åœ¨æˆ‘ä»¬é€šè¿‡get()å…¥å£ï¼Œè‡ªé¡¶å‘ä¸‹è§‚å¯Ÿå‡½æ•°ä¹‹é—´çš„è°ƒç”¨

```python
provider =  slim.dataset_data_provider.DatasetDataProvider(
                    dataset,
                    num_readers=FLAGS.num_readers,
                    common_queue_capacity=20 * FLAGS.batch_size,
                    common_queue_min=10 * FLAGS.batch_size,
                    shuffle=True)
===========================================================
|[image,labels] = provider.get(['image','label'])  # å•ŠçœŸè´¹äº‹ã€‚
|--key, data = parallel_reader.parallel_read(....)
|--items = dataset.decoder.list_items()
|--tensors = dataset.decoder.decode(data, items)
|--|--decode(self, serialized_example, items=None)
|--|--|--example = parsing_ops.parse_single_example(serialized_example,
                                                    self._keys_to_features)
|--|--|--|--outputs.append(handler.tensors_to_item(keys_to_tensors))
|--|--|--|--return outputs # ç”¨handlerå¤„ç†è§£æåtensorçš„ç»“æœ
|
|--items_to_tensors = dict(zip(items, tensors))
|--return [self._items_to_tensors[item] for item in items]
===========================================================
dataset.decoder
===========================================================
|dataset= slim.dataset.Dataset(
            data_sources=file_pattern,  # ./tfrecords/voc2007_train_*.tfrecord
            reader=reader,  # reader = tf.TFRecordReader
            decoder=decoder,  # ?
            num_samples=split_to_sizes[split_name],
            items_to_descriptions=items_to_descriptions,
            num_classes=num_classes,
            labels_to_names=labels_to_names)
|--decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)
|----keys_to_features
|----items_to_handlers
```



## è®­ç»ƒå‰ç»†èŠ‚

### 2.1Anchorsçš„ç”Ÿæˆ

ä¸ºæ‰€æœ‰ç‰¹å¾å›¾ç”Ÿæˆanchorsï¼šlayers_anchors: [(y,x,h,w),(y,x,h,w),(y,x,h,w),(y,x,h,w),(y,x,h,w),(y,x,h,w)]ã€‚å…ƒç´ ä¸ºtupleç±»å‹ã€‚

```python
def ssd_anchors_all_layers(img_shape,
                           layers_shape,
                           anchor_sizes,
                           anchor_ratios,
                           anchor_steps,
                           offset=0.5,
                           dtype=np.float32):
    """Compute anchor boxes for all feature layers.
    """
    layers_anchors = []
    for i, s in enumerate(layers_shape):  # ä¾‹å¦‚ç¬¬ä¸€å±‚
        anchor_bboxes = ssd_anchor_one_layer(img_shape, s,  # (38,38)
                                             anchor_sizes[i],  # (21,45)
                                             anchor_ratios[i],  # [2,.5]
                                             anchor_steps[i],  # 8
                                             offset=offset, dtype=dtype)
        layers_anchors.append(anchor_bboxes)
    return layers_anchors
```

ä¸‹é¢å‡½æ•°è¿”å›ï¼š

yï¼Œxï¼šå½¢çŠ¶ï¼š(H, W, 1)

hï¼Œw : å½¢çŠ¶ï¼š(num_anchors, ) æ¯”å¦‚ï¼š4ä¸ªæˆ–6ä¸ªï¼Œä¹Ÿè¦å½’ä¸€åŒ–


```python
def ssd_anchor_one_layer(img_shape,
                         feat_shape,
                         sizes,
                         ratios,
                         step,
                         offset=0.5,
                         dtype=np.float32):
    """Computer SSD default anchor boxes for one feature layer.
    Determine the relative position grid of the centers, and the relative
    width and height.
    ä»¥sdd300çš„ç¬¬ä¸€ä¸ªç‰¹å¾å›¾ä¸ºä¾‹:
    Arguments:
      feat_shape: Feature shape, used for computing relative position grids;  (38,38)
      size: Absolute reference sizes;  (21.,45.)
      ratios: Ratios to use on these features;  [2,.5]
      img_shape: Image shape, used for computing height, width relatively to the
        former;  (300,300)
      offset: Grid offset.  0.5
    Return:
      y, x, h, w: Relative x and y grids of the centers, and relative height and width.
    """
    # Weird SSD-Caffe computation using steps values...
    y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]
    y = (y.astype(dtype) + offset) * step / img_shape[0] 
    # y.astype(dtype):Copy of the array, cast to a specified type.
    x = (x.astype(dtype) + offset) * step / img_shape[1] # é™¤ä»¥300/8â‰ˆ38ï¼Œå³å½’ä¸€åŒ–
    
    # Expand dims to support easy broadcasting.
    y = np.expand_dims(y, axis=-1)
    x = np.expand_dims(x, axis=-1)

    # Compute relative height and width.
    # Tries to follow the original implementation of SSD for the order.
    num_anchors = len(sizes) + len(ratios)
    h = np.zeros((num_anchors, ), dtype=dtype)  
    w = np.zeros((num_anchors, ), dtype=dtype) 
    # Add first anchor boxes with ratio=1.
    h[0] = sizes[0] / img_shape[0] # 21/300 é»˜è®¤ratio=1çš„ä¸€ä¸ªæ¡†
    w[0] = sizes[0] / img_shape[1] # 21/300
    di = 1
    if len(sizes) > 1:
        h[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[0]  # é»˜è®¤ratio=æ›´å·ä¸‹s0*s1çš„æ¡†
        w[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[1]
        di += 1
    for i, r in enumerate(ratios):  #  å…¶ä»–2æˆ–è€…4ä¸ªæ¡†
        h[i+di] = sizes[0] / img_shape[0] / math.sqrt(r)
        w[i+di] = sizes[0] / img_shape[1] * math.sqrt(r)
    return y, x, h, w
```

å…¶ä¸­ï¼š

#### np.mgrid

ç›¸å½“äºç”Ÿæˆäº†ä¸€ä¸ªåæ ‡(x,y)ç½‘æ ¼ï¼Œåˆ†åˆ«å­˜å‚¨æ¨ªåæ ‡å’Œçºµåæ ‡ï¼Œè€Œä¸”æ­£å¥½ä¸å›¾åƒçš„å¯¹åº”ï¼ŒåŸç‚¹åœ¨å·¦ä¸Šè§’ã€‚

```python
import numpy as np
a = np.mgrid[0:5, 0:5]  # np.mgrid[0:a,0:b] ç”Ÿæˆ(2,a,b)å½¢çŠ¶çš„æ•°ç»„ï¼Œæ•°å­—æ’åˆ—è§„åˆ™è§ä¾‹å­ã€‚å…ˆ|åâ€”â€”
y, x = a
print(a)
print(a.shape) # (2, 5, 5)
print(y)
>>>
[
y [[0 0 0 0 0]
  [1 1 1 1 1]
  [2 2 2 2 2]
  [3 3 3 3 3]
  [4 4 4 4 4]]

x [[0 1 2 3 4]
  [0 1 2 3 4]
  [0 1 2 3 4]
  [0 1 2 3 4]
  [0 1 2 3 4]]
]
```

#### np.expand_dims

æ­£å¦‚ä½œè€…æ‰€è¯´ï¼šExpand dims to support easy broadcasting.

```python
y = np.expand_dims(y, axis=-1) # å¢åŠ ä¸€ä¸ªaxisç»´åº¦ (5,5)->(5,5,1)
print(y)
>>>
[[[0][0][0][0][0]]
[[1][1][1][1][1]]
[[2][2][2][2][2]]
[[3][3][3][3][3]]
[[4][4][4][4][4]]]
```

####  parallel_reader

```python
from tensorflow.contrib.slim.python.slim.data import parallel_reader
data_files = parallel_reader.get_data_files(data_sources)
```



### 2.2é¢„å¤„ç†å›¾åƒå¢å¼º

æ¥è‡ªproviderçš„æ•°æ®ï¼Œåç§°å’Œå½¢çŠ¶ï¼š

+ imageï¼š(Hï¼ŒWï¼Œ3)
+ glablesï¼šï¼ˆnum_objectsï¼Œï¼‰
+ gbboxesï¼š(Nï¼Œ4)

```python
def preprocess_for_train(image, labels, bboxes,
                         out_shape, data_format='NHWC',
                         scope='ssd_preprocessing_train'):
    """Preprocesses the given image for training.

    Note that the actual resizing scale is sampled from
        [`resize_size_min`, `resize_size_max`].

    Args:
        image: A `Tensor` representing an image of arbitrary size.
        output_height: The height of the image after preprocessing.
        output_width: The width of the image after preprocessing.
        resize_side_min: The lower bound for the smallest side of the image for
            aspect-preserving resizing.
        resize_side_max: The upper bound for the smallest side of the image for
            aspect-preserving resizing.

    Returns:
        A preprocessed image.
    """
    fast_mode = False
    with tf.name_scope(scope, 'ssd_preprocessing_train', [image, labels, bboxes]):
        if image.get_shape().ndims != 3:
            raise ValueError('Input must be of size [height, width, C>0]')
        # Convert to float scaled [0, 1].
        if image.dtype != tf.float32:
            image = tf.image.convert_image_dtype(image, dtype=tf.float32)
        tf_summary_image(image, bboxes, 'image_with_bboxes')

        # Distort image and bounding boxes.
        dst_image = image
        dst_image, labels, bboxes, distort_bbox = \
            distorted_bounding_box_crop(image, labels, bboxes,
                                        min_object_covered=MIN_OBJECT_COVERED,
                                        aspect_ratio_range=CROP_RATIO_RANGE)
        # Resize image to output size.
        dst_image = tf_image.resize_image(dst_image, out_shape,
                                          method=tf.image.ResizeMethod.BILINEAR,
                                          align_corners=False)
        tf_summary_image(dst_image, bboxes, 'image_shape_distorted')

        # Randomly flip the image horizontally.
        dst_image, bboxes = tf_image.random_flip_left_right(dst_image, bboxes)

        # Randomly distort the colors. There are 4 ways to do it.
        dst_image = apply_with_random_selector(
                dst_image,
                lambda x, ordering: distort_color(x, ordering, fast_mode),
                num_cases=4)
        tf_summary_image(dst_image, bboxes, 'image_color_distorted')

        # Rescale to VGG input scale.
        image = dst_image * 255.
        image = tf_image_whitened(image, [_R_MEAN, _G_MEAN, _B_MEAN])
        # Image data format.
        if data_format == 'NCHW':
            image = tf.transpose(image, perm=(2, 0, 1))
        return image, labels, bboxes
```

### 2.3 encode bboxï¼šçœŸæ­£è¾“å…¥

æœ€å¼€å§‹provideræä¾›çš„æ ‡ç­¾åªæ˜¯æ¯ä¸ªobjectçš„æ ‡ç­¾ï¼Œè€Œæˆ‘ä»¬è¦åšçš„æ˜¯ç»™æ‰€æœ‰anchorséƒ½å®‰æ’ä¸Šã€‚

`class SSDNet`ä¸­çš„æ–¹æ³•ã€‚

å¯¹äºåŸå§‹å›¾åƒå’Œé¢„å¤„ç†åçš„å›¾åƒï¼Œæˆ‘ä¹ˆéƒ½éœ€è¦ä¸º**æ¯ä¸€ä¸ªç‰¹å¼ å›¾çš„æ¯ä¸€ä¸ªanchoræ‰“æ­£è´Ÿæ ‡ç­¾**ï¼Œå¹¶ä¸”æŠŠåŸå§‹çš„bboxåæ ‡[ymin,xmin,ymax,xmin]ç¼–ç ä¸ºå¯ä»¥ç”¨äºå›å½’çš„åç§»é‡å“¦[cxï¼Œcyï¼Œhï¼Œw]ã€‚è´Ÿæ ·æœ¬çš„ç±»åˆ«æ˜¯0ï¼Œåç§»é‡ä¹Ÿæ˜¯é»˜è®¤çš„0ã€‚

```python
def bboxes_encode(self, labels, bboxes, anchors,
                  scope=None):
    """Encode labels and bounding boxes.
    """
    return ssd_common.tf_ssd_bboxes_encode(
        labels, bboxes, anchors,
        self.params.num_classes,
        self.params.no_annotation_label,
        ignore_threshold=0.5,
        prior_scaling=self.params.prior_scaling,
        scope=scope)
```

`tf_ssd_bboxes_encode`å®šä¹‰åœ¨`nets/ssd_common.py`:

è¾“å…¥ä¸ºï¼š

é¢„å¤„ç†åçš„(1)å›¾åƒæ•°æ®å’Œ(2)ç”Ÿæˆçš„anchor

+ ~~imageï¼š(Wï¼ŒHï¼Œ3)~~

+ glables: Tensorï¼ˆnum_objectsï¼Œï¼‰
+ gbboxesï¼šTensor (num_objectsï¼Œ4)
+ ssd_anchorsï¼šList [(yï¼Œxï¼Œwï¼Œh) Ã— 6â€¦â€¦]ï¼Œå…­ä¸ªç‰¹å¾å›¾ã€‚
  + yï¼Œxï¼šndarrayï¼Œå½¢çŠ¶ï¼š(H, W, 1)ï¼Œæ¡†ä¸­å¿ƒå½’ä¸€åŒ–åæ ‡ã€‚
  + hï¼Œw : ndarrayï¼Œå½¢çŠ¶ï¼š(num_anchors, ) æ¯”å¦‚ï¼š4ä¸ªæˆ–6ä¸ªï¼Œä¹Ÿè¦å½’ä¸€åŒ–

è¿”å›ï¼šéƒ½æ˜¯Listï¼Œé•¿åº¦éƒ½ä¸º6ï¼Œå¯¹åº”6ä¸ªä¸åŒçš„ç‰¹å¾å›¾ï¼Œsize_featuresä¸åŒã€‚

+ gclassesï¼šListï¼Œå…ƒç´ ä¸ºï¼š
  + target_labelsï¼šTensorï¼ˆsize_featuresï¼Œsize_featuresï¼Œ4/6ï¼‰

+ glocalisationsï¼šList
  +  target_localizationsï¼šTensorï¼ˆsize_featuresï¼Œsize_featuresï¼Œ4/6ï¼Œ4ï¼‰
+ gscores ï¼šList
  + target_scoresï¼šTensorï¼ˆsize_featuresï¼Œsize_featuresï¼Œ4/6ï¼‰

```python
# Encoding boxes for all feature layers.
def tf_ssd_bboxes_encode(labels,
                         bboxes,
                         anchors,
                         num_classes,
                         no_annotation_label,
                         ignore_threshold=0.5,
                         prior_scaling=[0.1, 0.1, 0.2, 0.2],
                         dtype=tf.float32,
                         scope='ssd_bboxes_encode'):
    """Encode groundtruth labels and bounding boxes using SSD net anchors.
    Encoding boxes for all feature layers.

    Arguments:
      labels: 1D Tensor(int64) containing groundtruth labels;
      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;
      anchors: List of Numpy array with layer anchors;
      matching_threshold: Threshold for positive match with groundtruth bboxes;
      prior_scaling: Scaling of encoded coordinates.

    Return:
      (target_labels, target_localizations, target_scores):
        Each element is a list of target Tensors.
    """
    with tf.name_scope(scope):
        target_labels = []
        target_localizations = []
        target_scores = []
        for i, anchors_layer in enumerate(anchors):
            with tf.name_scope('bboxes_encode_block_%i' % i):
                t_labels, t_loc, t_scores = \
                    tf_ssd_bboxes_encode_layer(labels, bboxes, anchors_layer,
                                               num_classes, no_annotation_label,
                                               ignore_threshold,
                                               prior_scaling, dtype)
                target_labels.append(t_labels)
                target_localizations.append(t_loc)
                target_scores.append(t_scores)
        return target_labels, target_localizations, target_scores
```



```python
# Encoding boxes for one layers.
def tf_ssd_bboxes_encode_layer(labels,
                               bboxes,
                               anchors_layer,
                               num_classes,
                               no_annotation_label,
                               ignore_threshold=0.5,
                               prior_scaling=[0.1, 0.1, 0.2, 0.2],
                               dtype=tf.float32):
    """Encode groundtruth labels and bounding boxes using SSD anchors from
    one layer.

    Arguments:
      labels: 1D Tensor(int64) containing groundtruth labels;
      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;
      anchors_layer: Numpy array with one layer's anchors;
      matching_threshold: Threshold for positive match with groundtruth bboxes;
      prior_scaling: Scaling of encoded coordinates.

    Return:
      (target_labels, target_localizations, target_scores): Target Tensors.
    """
    # Anchors coordinates and volume.
    yref, xref, href, wref = anchors_layer  # è¿™äº›å€¼éƒ½ç»è¿‡äº†å½’ä¸€åŒ–ã€‚
    # yrefå’Œxrefçš„shapeä¸ºï¼ˆ38,38,1ï¼‰ï¼›hrefå’Œwrefçš„shapeä¸ºï¼ˆ4ï¼Œï¼‰ï¼Œå¯ä»¥è¿ç®—å—ï¼Ÿ
    # å¯ä»¥ï¼Œå®éªŒè¯æ˜å½¢çŠ¶ç”±ï¼ˆ38,38,1ï¼‰=>ï¼ˆ38,38,4ï¼‰
    ymin = yref - href / 2.  # ç”±ä¸­å¿ƒç‚¹æ‰¾å·¦ä¸Šã€å³ä¸‹åæ ‡
    xmin = xref - wref / 2.
    ymax = yref + href / 2.
    xmax = xref + wref / 2.
    # ç”±å·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡ç¡®å®šanchoré¢ç§¯ï¼Œç”¨äºä¹‹åç®—IoU
    vol_anchors = (xmax - xmin) * (ymax - ymin)

    # Initialize tensors...
    shape = (yref.shape[0], yref.shape[1], href.size)  # æ¯”å¦‚ï¼Œç¬¬ä¸€å±‚(38,38,4)
    # åˆå§‹åŒ–ç‰¹å¾å›¾ä¸Šçš„ç‚¹å¯¹åº”çš„å„ä¸ªanchoræ‰€å±  æ ‡ç­¾ç±»åˆ«å’Œåˆ†æ•°ï¼ˆæ‰“æ ‡ç­¾ï¼‰
    feat_labels = tf.zeros(shape, dtype=tf.int64)  # åˆå§‹å€¼0
    feat_scores = tf.zeros(shape, dtype=dtype)
    # åˆå§‹åŒ–boxå¯¹åº”çš„ground truthçš„åæ ‡ï¼ˆæ‰“æ ‡ç­¾ï¼‰
    feat_ymin = tf.zeros(shape, dtype=dtype)    #ï¼ˆ38ï¼Œ38ï¼Œ4ï¼‰
    feat_xmin = tf.zeros(shape, dtype=dtype)
    feat_ymax = tf.ones(shape, dtype=dtype)
    feat_xmax = tf.ones(shape, dtype=dtype)

    def jaccard_with_anchors(bbox):
        """Compute jaccard score between a box and the anchors.
        ä¼ å…¥çš„æ˜¯ground truthçš„bbox
        """
        int_ymin = tf.maximum(ymin, bbox[0])   # å·¦ä¸Šè§’é€‰å¤§å€¼
        int_xmin = tf.maximum(xmin, bbox[1])
        int_ymax = tf.minimum(ymax, bbox[2])   # å³ä¸‹è§’é€‰å°å€¼
        int_xmax = tf.minimum(xmax, bbox[3])
        h = tf.maximum(int_ymax - int_ymin, 0.)  # å¯èƒ½ä¸é‡å ï¼Œä¼šä¸ºè´Ÿå€¼
        w = tf.maximum(int_xmax - int_xmin, 0.)
        # Volumes.
        inter_vol = h * w
        union_vol = vol_anchors - inter_vol \
            + (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])  # å¹¶ é¢ç§¯
        jaccard = tf.div(inter_vol, union_vol)
        return jaccard

    # è¿™ä¸ªä¸æ˜¯IoU(å¹¶é›†)ï¼Œæ˜¯Io anchorsï¼Œè¿™é‡Œæ²¡ç”¨
    def intersection_with_anchors(bbox):
        """Compute intersection between score a box and the anchors.
        """
        int_ymin = tf.maximum(ymin, bbox[0])
        int_xmin = tf.maximum(xmin, bbox[1])
        int_ymax = tf.minimum(ymax, bbox[2])
        int_xmax = tf.minimum(xmax, bbox[3])
        h = tf.maximum(int_ymax - int_ymin, 0.)
        w = tf.maximum(int_xmax - int_xmin, 0.)
        inter_vol = h * w
        scores = tf.div(inter_vol, vol_anchors)
        return scores

    def condition(i, feat_labels, feat_scores,
                  feat_ymin, feat_xmin, feat_ymax, feat_xmax):
        """Condition: check label index.
        """
        # groundtruth labels
        # éå†æ‰€æœ‰ground truthçš„æ¡†
        # å³iä¸labelsçš„ç¬¬ä¸€ç»´å¯¹æ¯”ï¼ˆå®é™…ä¸Šlabelså°±æ˜¯1D Tesnorï¼‰ï¼Œå³æœ‰å‡ ä¸ªæ¡†ï¼Œéå†å‡ ä¸ªæ¡†
        # if i < tf.shape(labels)[0]
        r = tf.less(i, tf.shape(labels))
        return r[0]

    # è¯¥å‡½æ•°å¤§è‡´æ„æ€æ˜¯é€‰æ‹©ä¸gt box IOUæœ€å¤§çš„é”šç‚¹æ¡†è´Ÿè´£è¯¥gtboxçš„å›å½’ä»»åŠ¡ï¼Œ
    # æ‰€ä»¥è¦å¾ªç¯æ‰¾å‡ºåŒ¹é…æ¡†
    def body(i, feat_labels, feat_scores,
             feat_ymin, feat_xmin, feat_ymax, feat_xmax):
        """Body: update feature labels, scores and bboxes.
        Follow the original SSD paper for that purpose:
          - assign values when jaccard > 0.5;
          - only update if beat the score of other bboxes.
        """
        # Jaccard score.
        label = labels[i]
        bbox = bboxes[i]
        # è¿”å›çš„æ˜¯äº¤å¹¶æ¯”,ç®—ä¸€å±‚ç‰¹å¾å›¾ä¸Šæ‰€æœ‰çš„æ¡†å’Œå›¾åƒä¸­ç¬¬ä¸ªiä¸ªground truthçš„äº¤å¹¶æ¯”
        jaccard = jaccard_with_anchors(bbox)
        # Mask: check threshold + scores + no annotations + num_classes. å¸ƒå°”å€¼
        # æ¯æ¬¡æ¯”è¾ƒç¬¬iä¸ªground truthä¸æ‰€æœ‰anchorçš„IoUä¸ä¸Šæ¬¡å¾ªç¯i-1 gtçš„feat_score
        # æ¯”ä¸Šæ¬¡å¤§çš„ç½®ä¸º Trueï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªä½ç½®çš„anchorå±äºè¿™æ¬¡çš„ground truthï¼Œè€Œä¸æ˜¯ä¸Šæ¬¡çš„ã€‚
        # æ‰€ä»¥éœ€è¦æ›´æ–°ä¸º ç¬¬iä¸ªground truthçš„ labelã€scoreså’Œåæ ‡
        # feat_scoresåˆå§‹å€¼æ˜¯å°±æ˜¯0ï¼Œæ‰€ä»¥è¿™å—å·²ç»æŠŠå°äº0çš„ç­›é€‰æ‰ï¼Œä¸å¯èƒ½æœ‰è´Ÿåˆ†å•Š
        mask = tf.greater(jaccard, feat_scores)  # å½¢çŠ¶è¿˜æ˜¯ï¼ˆ38ï¼Œ38ï¼Œ4ï¼‰ï¼Œè®°ä½è¿™æ˜¯å¾ªç¯
        # mask = tf.logical_and(mask, tf.greater(jaccard, matching_threshold))
        mask = tf.logical_and(mask, feat_scores > -0.5)  
        # è¿™å—å’‹ä¸é€‰å¤§äº0.5çš„???è¿™ç‰¹ä¹ˆçš„æœ‰é—®é¢˜å•Šï¼Œä¸Šä¸Šä¸€å¥å·²ç»æŠŠå°äº0çš„æ’é™¤äº†å•Šã€‚
        # çœ‹æ¥æ˜¯åªè¦æœ‰IoU>0ï¼Œè¿™é‡Œå°±ç»™æ‰“æ ‡ç­¾ã€‚åœ¨è®¡ç®—æŸå¤±çš„æ—¶å€™å†è€ƒè™‘æ­£è´Ÿæ ·æœ¬çš„é—®é¢˜
        # æ‰€ä»¥è¦ä¼ å…¥scoresåˆ°ç½‘ç»œä¸­ï¼Œå†è®¡ç®—æŸå¤±çš„æ—¶å€™ç”¨æ¥ç­›é€‰å‡ºæ­£æ ·æœ¬
        mask = tf.logical_and(mask, label < num_classes)
        imask = tf.cast(mask, tf.int64)  # ç”¨äºæ•´å‹è®¡ç®—
        fmask = tf.cast(mask, dtype)  # ç”¨äºæµ®ç‚¹å‹è®¡ç®—
        # Update values using mask. æ›´æ–°
        feat_labels = imask * label + (1 - imask) * feat_labels  # 1
        feat_scores = tf.where(mask, jaccard, feat_scores)  # 2

        # é€‰æ‹©ä¸å…¶IOUæœ€å¤§çš„GT bboxä½œä¸ºå›å½’ç›®æ ‡  # 3
        feat_ymin = fmask * bbox[0] + (1 - fmask) * feat_ymin
        feat_xmin = fmask * bbox[1] + (1 - fmask) * feat_xmin
        feat_ymax = fmask * bbox[2] + (1 - fmask) * feat_ymax
        feat_xmax = fmask * bbox[3] + (1 - fmask) * feat_xmax

        # Check no annotation label: ignore these anchors...
        # interscts = intersection_with_anchors(bbox)
        # mask = tf.logical_and(interscts > ignore_threshold,
        #                       label == no_annotation_label)
        # # Replace scores by -1.
        # feat_scores = tf.where(mask, -tf.cast(mask, dtype), feat_scores)

        return [i+1, feat_labels, feat_scores,
                feat_ymin, feat_xmin, feat_ymax, feat_xmax]
    # Main loop definition.
    i = 0
    [i, feat_labels, feat_scores,
     feat_ymin, feat_xmin,
     feat_ymax, feat_xmax] = tf.while_loop(condition, body,
                                           [i, feat_labels, feat_scores,
                                            feat_ymin, feat_xmin,
                                            feat_ymax, feat_xmax])
    # Transform to center / size.(38,38,4)
    feat_cy = (feat_ymax + feat_ymin) / 2.
    feat_cx = (feat_xmax + feat_xmin) / 2.
    feat_h = feat_ymax - feat_ymin
    feat_w = feat_xmax - feat_xmin
    # Encode features.è¿™æ‰æ˜¯çœŸæ­£è¦å›å½’çš„ç›®æ ‡
    feat_cy = (feat_cy - yref) / href / prior_scaling[0]
    feat_cx = (feat_cx - xref) / wref / prior_scaling[1]
    feat_h = tf.log(feat_h / href) / prior_scaling[2]
    feat_w = tf.log(feat_w / wref) / prior_scaling[3]
    # Use SSD ordering: x / y / w / h instead of ours.
    feat_localizations = tf.stack([feat_cx, feat_cy, feat_w, feat_h], axis=-1)
    return feat_labels, feat_localizations, feat_scores
```

## è®­ç»ƒ

### 3.1batch

```python
# å¼„æ‡‚reshpe_listè¦å¹²å˜›ï¼Œå°±çŸ¥é“è¿”å›rçš„åˆ—è¡¨é•¿åº¦ä¸º1+6+6+6,è¿™ä¸ªæ“ä½œä¹‹åå°±å¤šäº†ä¸ªbatch_sizeç»´åº¦
r = tf.train.batch(
    tf_utils.reshape_list([image, gclasses, glocalisations, gscores]),
    batch_size=FLAGS.batch_size,
    num_threads=FLAGS.num_preprocessing_threads,
    capacity=5 * FLAGS.batch_size)
# ç­‰runæ“ä½œå®Œæˆï¼Œå†reshapeå›å»ã€‚æ³¨æ„ï¼šå¢åŠ äº†ç¬¬ä¸€ç»´åº¦batch_size
b_image, b_gclasses, b_glocalisations, b_gscores = \
                tf_utils.reshape_list(r, batch_shape)
```

ä¸ºäº†ä¾¿äºrunï¼Œç”¨åˆ°TFä¸­runçš„æ“ä½œï¼Œè¦æŠŠlistä¸­çš„Tensoråˆ†å¼€ã€‚

> [Tensor,[Tesnor1_1,Tesnor1_2],[Tesnor2_1,Tensor2_2]]----------->>>>
>
> [Tensor,Tesnor1_1,Tesnor1_2,Tesnor2_1,Tensor2_2]

ä¸ºäº†ä¾¿äºæ“ä½œï¼Œè¿ç®—æ—¶åˆæŠŠ6ç‰¹å¾å›¾ä¸Šçš„æ•°æ®stackæˆä¸€ä¸ªlistã€‚

```python
def reshape_list(l, shape=None):
    '''
    shape = None
    è¦ä¿è¯tf.train.batchä¼ å…¥çš„éƒ½æ˜¯å¯ä»¥runçš„ï¼Œè€Œä¸æ˜¯listç±»å‹ï¼ˆæ²¡æ³•runï¼‰ã€‚ç›¸å½“äºè¿æ¥æ“ä½œï¼Œ
    è€Œgclasses, glocalisations, gscoreséƒ½æ˜¯listï¼ŒæŠŠä»–ä»¬è¿æ¥æˆä¸€ä¸ªlistã€‚
    shape is not Noneï¼Œåˆ™æ˜¯ç›¸åçš„æ“ä½œã€‚
    '''
    r = []
    if shape is None:
        # Flatten everything.
        for a in l:
            if isinstance(a, (list, tuple)):
                r = r + list(a)  # æ¯”å¦‚['image']+[1,1] = ['iamge',1,1]
            else:
                r.append(a)
    else:
        i = 0
        for s in shape:
            if s == 1:
                r.append(l[i])
            else:
                r.append(l[i:i+s])
            i += s
    return r
```

ç½‘ç»œä¸»ä½“éƒ¨åˆ†ï¼š

```python
batch_queue = slim.prefetch_queue.prefetch_queue(
                tf_utils.reshape_list([b_image, b_gclasses, b_glocalisations, b_gscores]),
                capacity=2 * deploy_config.num_clones)
def clone_fn(batch_queue):
    """Allows data parallelism by creating multiple
    clones of network_fn."""
    # 1.å–å‡ºä¸€æ‰¹æ•°æ®ã€‚Dequeue batch.
    b_image, b_gclasses, b_glocalisations, b_gscores = \
        tf_utils.reshape_list(batch_queue.dequeue(), batch_shape)

    # 2.arg_scopeæ„å»ºç½‘ç»œå¹¶ä¸”çš„æ­é…è¾“å‡ºï¼Œä¸ºä»€ä¹ˆè¯´arg_scopeæ˜¯ç”¨æ¥æ„å»ºç½‘ç»œå‘¢ï¼Ÿâ†“â†“â†“
    arg_scope = ssd_net.arg_scope(weight_decay=FLAGS.weight_decay,
                                  data_format=DATA_FORMAT)
    with slim.arg_scope(arg_scope):
        predictions, localisations, logits, end_points = \
            ssd_net.net(b_image, is_training=True)
    # 3.Add loss function.
    # æ¯ä¸ªlosséƒ½ä¼šä¿å­˜åœ¨tf.GraphKeys.LOSSESï¼Œæ‰€ä»¥è¿™ä¸ªlossesæ–¹æ³•æ²¡æœ‰è¿”å›ä»»ä½•ä¸œè¥¿
    # ä¸æ˜¯ç›´æ¥è¿”å›ç»™ä¸€ä¸ªTensorï¼Œå…·ä½“çœ‹3.2èŠ‚
    ssd_net.losses(logits, localisations,
                   b_gclasses, b_glocalisations, b_gscores,
                   match_threshold=FLAGS.match_threshold,
                   negative_ratio=FLAGS.negative_ratio,
                   alpha=FLAGS.loss_alpha,
                   label_smoothing=FLAGS.label_smoothing)
    return end_points
```

1. è¿™é‡Œç”¨åˆ°äº†`slim.prefetch_queue.prefetch_queue`æ–¹æ³•[ğŸ‘‰](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/prefetch_queue.py)ï¼š

    > Creates a queue to prefetch tensors from `tensors`.
    >
    > A queue runner for enqueuing tensors into the prefetch_queue is automatically added to the TF QueueRunners collection.

    è°ƒç”¨è¿™ä¸ªæ–¹æ³•ï¼Œå°±åˆ›é€ äº†ä¸€ä¸ªé¢„å–Tensorsçš„é˜Ÿåˆ—ï¼ŒèŠ‚çœäº†æ¯æ¬¡å–æ•°æ®ç»„è£…çš„æ—¶é—´ã€‚ä¾‹å¦‚ï¼š

    ```python
    images, labels = tf.train.batch([image, label], batch_size=32, num_threads=4)
    batch_queue = prefetch_queue([images, labels])
    images, labels = batch_queue.dequeue()
    logits = Net(images)
    loss = Loss(logits, labels)QueueRunnerè¢«æ·»åŠ ã€‚ç›®å‰ä¸äº†è§£è¿™ä¸ªã€‚ã€‚ã€‚ã€‚
    ```

    [TensorFlowä¸­çš„Queueå’ŒQueueRunner](https://zhuanlan.zhihu.com/p/31361295)

    [Tensorflowæ·±åº¦å­¦ä¹ ä¹‹åä¸ƒï¼šé˜Ÿåˆ—ä¸å¤šçº¿ç¨‹](https://blog.csdn.net/DaVinciL/article/details/77342027)

2. slim.arg_scopeæ˜¯slimåº“çš„ç‰¹è‰²ï¼ŒæŠŠå„ç§å±‚çš„ç›¸åŒå‚æ•°çš„å®šä¹‰æ”¾åˆ°ä¸€èµ·ï¼Œè¿™æ ·å°±ä¸ç”¨åœ¨ç½‘ç»œå®šä¹‰æ—¶ç”³æ˜è¿™äº›å‚æ•°äº†ï¼Œ**ä½†å½“ç”¨åˆ°ç½‘ç»œæ—¶ï¼Œå¿…é¡»æŒ‡å®š**`slim.arg_scope`

   ```python
   def ssd_arg_scope(weight_decay=0.0005, data_format='NHWC'):
       """Defines the VGG arg scope.
       Args:
         weight_decay: The l2 regularization coefficient.
       Returns:
         An arg_scope.
       """
       with slim.arg_scope([slim.conv2d, slim.fully_connected],
                           activation_fn=tf.nn.relu,
                           weights_regularizer=slim.l2_regularizer(weight_decay),
                           weights_initializer=tf.contrib.layers.xavier_initializer(),
                           biases_initializer=tf.zeros_initializer()):
           with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                               padding='SAME',
                               data_format=data_format):
               with slim.arg_scope([custom_layers.pad2d,
                                    custom_layers.l2_normalization,
                                    custom_layers.channel_to_last],
                                   data_format=data_format) as sc:
                   return sc
   ```

3. Add loss function.è§3.2èŠ‚ã€‚

### 3.2loss

è®ºæ–‡ä¸­æ˜¯å¤šä»»åŠ¡æŸå¤±ï¼Œè¿˜éœ€è¦å°†æ­£è´Ÿçš„æ¯”ä¾‹æ§åˆ¶åœ¨1ï¼š3ï¼Œè¿˜è¦å¯¹è´Ÿæ ·æœ¬æŒ‰åˆ†æ•°è¿›è¡Œæ’åºï¼ˆè¿™ä¸ªåˆ†æ•°å¥½åƒå°±æ˜¯æŒ‰ç…§IoUè®¡ç®—çš„ï¼Œè¶Šå°è¯´æ˜æ˜¯èƒŒæ™¯çš„ç½®ä¿¡åˆ†æ•°è¶Šé«˜ï¼‰ï¼Œè¶Šè´Ÿè¶Šå¥½ã€‚

```python
ssd_net.losses(logits, localisations,
                   b_gclasses, b_glocalisations, b_gscores,
                   match_threshold=FLAGS.match_threshold,  # 0.5
                   negative_ratio=FLAGS.negative_ratio,  # 3.
                   alpha=FLAGS.loss_alpha,  # 1.0
                   label_smoothing=FLAGS.label_smoothing)  # 0
```

å‚æ•°ä¸­ï¼š

+ ç½‘ç»œçš„è¾“å‡ºé¢„æµ‹ï¼šlogits(æ²¡ç»è¿‡softmax)ï¼š(N,H,W,4/6,21)Ã—6ï¼Œlocalisationsï¼š(N,H,W,4/6,4)ï¼Œæ³¨æ„è¿™æ˜¯5ç»´çš„ã€‚åœ¨ç½‘ç»œæœ€åçš„è™šåŒºï¼ˆè‡´æ•¬çŒ´å“¥ï¼‰ä¸Šè¿›è¡Œreshapeã€‚
+ encode bboxçš„è¾“å‡ºï¼Œå³æ ‡ç­¾ï¼šb_gclasses, b_glocalisations, b_gscoresã€‚å‰ä¸¤ä¸ªç”¨äºè®¡ç®—æŸå¤±ï¼Œåä¸€ä¸ªåˆ†æ•°ç”¨äºç»™è´Ÿæ ·æœ¬æ’åºï¼Œé€‰æ‹©ç”¨äºå›å½’çš„è´Ÿæ ·æœ¬ã€‚

ä»£ç å¥½é•¿é•¿é•¿é•¿é•¿ï¼š

```python
def ssd_losses(logits, localisations,
               gclasses, glocalisations, gscores,
               match_threshold=0.5,
               negative_ratio=3.,
               alpha=1.,
               label_smoothing=0.,
               device='/cpu:0',
               scope=None):
    '''Loss functions for training the SSD 300 VGG network.
    This function defines the different loss components of the SSD, and
    adds them to the TF loss collection.

    Arguments:
      logits: (list of) predictions logits Tensors;[N,H,W,num_anchor, 21]
      localisations: (list of) localisations Tensors;[N,H,W,num_anchor, 4]
      gclasses: (list of) groundtruth labels Tensors;
      glocalisations: (list of) groundtruth localisations Tensors;[N,H,W,num_anchor,4]
      gscores: (list of) groundtruth score Tensors; IOU???
      alpha: ä½ç½®è¯¯å·®æƒé‡ç³»æ•°
    '''
    with tf.name_scope(scope, 'ssd_losses'):
        lshape = tfe.get_shape(logits[0], 5)  # ç¬¬ä¸€ä¸ªç‰¹å¾å›¾: [N,38,38,num_anchor,21]
        num_classes = lshape[-1]  # 21
        batch_size = lshape[0]  # N

        # Flatten out all vectors!æ–¹ä¾¿è®¡ç®—ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
        flogits = []
        fgclasses = []
        fgscores = []
        flocalisations = []
        fglocalisations = []
        for i in range(len(logits)):  # éå†æ¯ä¸ªç‰¹å¾å›¾ï¼ŒFlattenæŒ‰ç€æ¯ä¸ªanchor
            flogits.append(tf.reshape(logits[i], [-1, num_classes]))#é¢„æµ‹(N*H*W*num_anchor,21)
            fgclasses.append(tf.reshape(gclasses[i], [-1]))  # çœŸå®ç±»åˆ«(N*H*W*num_anchor,)ï¼Œä¸æ˜¯one_hotç¼–ç 
            fgscores.append(tf.reshape(gscores[i], [-1]))  # é¢„æµ‹ç›®æ ‡å¾—åˆ†
            flocalisations.append(tf.reshape(localisations[i], [-1, 4])) #é¢„æµ‹è¾¹æ¡†åæ ‡
            fglocalisations.append(tf.reshape(glocalisations[i], [-1, 4])) #groundtruthçœŸå®åæ ‡
        # And concat the crap!æŠŠæ‰€æœ‰ç‰¹å¾å›¾æ”¾ä¸€ä¸ªlisté‡Œï¼ï¼
        logits = tf.concat(flogits, axis=0) # [1+2+3+4+5+6å±‚,21]
        gclasses = tf.concat(fgclasses, axis=0)
        gscores = tf.concat(fgscores, axis=0)
        localisations = tf.concat(flocalisations, axis=0)
        glocalisations = tf.concat(fglocalisations, axis=0)
        dtype = logits.dtype

        # Compute positive matching mask...
        # æ­£ä¾‹æ˜¯æ ¹æ®IoUç¡®å®šï¼Œè´Ÿæ ·æœ¬çš„é€‰æ‹©æ ¹æ®é¢„æµ‹çš„ç½®ä¿¡åˆ†
        pmask = gscores > match_threshold  # pmaskçš„é•¿åº¦è·Ÿgscoresä¸€æ · æ‰€æœ‰anchorsçš„æ•°é‡
        fpmask = tf.cast(pmask, dtype)
        n_positives = tf.reduce_sum(fpmask)  # æ­£æ ·æœ¬æ•°é‡æ˜¯è¿™ä¹ˆæ¥çš„

        # Hard negative mining...
        no_classes = tf.cast(pmask, tf.int32)
        predictions = slim.softmax(logits)
        # è´Ÿæ ·æœ¬å¸ƒå°”æ ‡å¿— 1è¡¨ç¤ºæ˜¯è´Ÿæ ·æœ¬çš„
        nmask = tf.logical_and(tf.logical_not(pmask),
                               gscores > -0.5)
        # ?????å¤§äº-0.5ä»€ä¹ˆé¬¼ï¼Œé¦–å…ˆnot pmaskä¸­å·²ç»æŠŠæ­£æ ·æœ¬å»æ‰ï¼Œ
        # gscoreæ˜¯åœ¨encode bboxè®¡ç®—çš„ï¼Œä¹Ÿæ²¡æœ‰è´Ÿçš„å•Š
        # è¦æ³¨æ„è¿™ä¸ªscoreç‰¹ä¹ˆæ€ä¹ˆç®—çš„2018/7/29
        # ç­”ï¼šç®—IoUï¼Œä¸¤ä¸ªä¸é‡å çš„æ¡†ä¼šå‡ºç°è´Ÿå€¼å“¦ï¼Œä½†æ˜¯è´Ÿå€¼è¢«ç­›é€‰æ‰äº†å•Š2018/8/3
        fnmask = tf.cast(nmask, dtype)
        nvalues = tf.where(nmask,
                           predictions[:, 0],# æ˜¯è´Ÿæ ·æœ¬ï¼Œå–å‡ºå¯¹åº”çš„é¢„æµ‹ç½®ä¿¡åˆ†æ•°
                           1. - fnmask)  # ä¸æ˜¯è´Ÿæ ·æœ¬ï¼Œ1-0=1
        # åœ¨æ‰“æ ‡ç­¾çš„æ—¶å€™å¹¶æ²¡æœ‰åŒºåˆ†æ­£è´Ÿæ ·æœ¬ï¼Œä¹Ÿå°±æ˜¯è¯´é™¤äº†é‚£äº›æ²¡æœ‰åŒ¹é…åˆ°çš„anchorï¼Œå‰©ä½™çš„
        # anchoréƒ½è¢«æ‰“ä¸Šç±»åˆ«æ ‡ç­¾ï¼Œæ— è®ºIoUæ˜¯å¦å¤§äº0.5ï¼Œç½®ä¿¡åˆ†æ•°ä»ç„¶æ˜¯IoUå¤§å°
        # æ‰€ä»¥ï¼Œåˆ†æ•°è¶Šå¤§ï¼Œé‚£ä¹ˆä¸ground truthçš„é‡å è¶Šå¤šã€‚å°±è¶Šhard
        nvalues_flat = tf.reshape(nvalues, [-1])
        # Number of negative entries to select.
        max_neg_entries = tf.cast(tf.reduce_sum(fnmask), tf.int32) # è´Ÿæ ·æœ¬æ•°é‡
        n_neg = tf.cast(negative_ratio * n_positives, tf.int32) + batch_size  # ä¸ºä»€ä¹ˆè¦åŠ batch_size
        n_neg = tf.minimum(n_neg, max_neg_entries)  # è¦ç”¨åˆ°çš„è´Ÿæ ·æœ¬æ•°é‡

        val, idxes = tf.nn.top_k(-nvalues_flat, k=n_neg) # ====å…³é”®===ä¸ºä»€ä¹ˆé€‰ç½®ä¿¡åº¦å°çš„çš„ï¼Ÿ
        max_hard_pred = -val[-1]  # è´Ÿæ ·æœ¬ä¸­æœ€å¤§å¾—åˆ†ï¼Œå¦‚[-0.1,-0.2,-0.5]->0.5
        # Final negative mask.
        nmask = tf.logical_and(nmask, nvalues < max_hard_pred)  # å°äºæœ€hardçš„è´Ÿæ ·æœ¬çš„åˆ†æ•°
        fnmask = tf.cast(nmask, dtype)

        # ========================æŸå¤±å‡½æ•°==================================
        # å¾—åˆ°pmaskï¼Œnmaskå’Œå¯¹åº”çš„æ•´æ•°å€¼1/0ï¼šfpmask,fnmask.
        # Add cross-entropy loss.
        with tf.name_scope('cross_entropy_pos'):
            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,
                                                                  labels=gclasses)
            # gclasses 21ç±»æ±‚äº¤å‰ç†µï¼Œä¹‹åå–fpmaskæ©ç ç®—æ­£ä¾‹çš„æŸå¤±
            # ä¸ºä»€ä¹ˆæ˜¯é™¤ä»¥batch_size???
            loss = tf.div(tf.reduce_sum(loss * fpmask), batch_size, name='value')
            tf.losses.add_loss(loss)
            
            # loss_n = tf.div(tf.reduce_sum(loss * fnmask), batch_size, name='value')
            # ä¸Šä¸€è¡Œä»£ç åº”è¯¥è·Ÿè´Ÿæ ·æœ¬æŸå¤±ä¸€æ ·å•Šï¼Œno_classesä¸fpmaskéƒ½æ˜¯æ•´å½¢æ ‡å¿—å•Š
            # no_classesç›¸å½“äºæŠŠæ­£æ ·æœ¬å…¨ç½®ä¸º1ï¼Œè´Ÿæ ·æœ¬ä¸º0ï¼Œ
            # åæ­£éƒ½æ˜¯ä»è´Ÿæ ·æœ¬ä¸­æŒ‰ç…§fnmaskæ ‡å¿—é€‰å‡ºç”¨äºå›å½’çš„è´Ÿæ ·æœ¬æŸå¤±å€¼ï¼Œè·Ÿç½®ä¸ç½®1æ²¡ä»€ä¹ˆå…³ç³»å•Š
            
        with tf.name_scope('cross_entropy_neg'):
            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,
                                                                  labels=no_classes)
            # no_classes 0/1ç±»åˆ«ã€‚ç®—èƒŒæ™¯çš„æŸå¤±
            loss = tf.div(tf.reduce_sum(loss * fnmask), batch_size, name='value')
            tf.losses.add_loss(loss)

        # Add localization loss: smooth L1, L2, ...
        with tf.name_scope('localization'):
            # Weights Tensor: positive mask + random negative.
            weights = tf.expand_dims(alpha * fpmask, axis=-1)
            # ä½ç½®æ˜¯æ¯”åˆ†æ•°å¤šäº†ä¸€ä¸ªç»´åº¦ã€‚(-1,4)
            loss = custom_layers.abs_smooth(localisations - glocalisations)
            loss = tf.div(tf.reduce_sum(loss * weights), batch_size, name='value')
            tf.losses.add_loss(loss)
```
Smooth L1å‡½æ•°ï¼š

```python
def abs_smooth(x):
    """Smoothed absolute function. Useful to compute an L1 smooth error.

    Define as:
        x^2 / 2         if abs(x) < 1
        abs(x) - 0.5    if abs(x) > 1
    We use here a differentiable definition using min(x) and abs(x). Clearly
    not optimal, but good enough for our purpose!
    """
    absx = tf.abs(x)
    minx = tf.minimum(absx, 1)
    r = 0.5 * ((absx - 1) * minx + absx)
    return r
```

#### 3.2.1æŸå¤±å‡½æ•°

ä»£ç ä¸­è¿™æ®µæ³¨é‡Šæ˜¯æˆ‘ç†è§£é”™äº†ï¼š

```python
# loss_n = tf.div(tf.reduce_sum(loss * fnmask), batch_size, name='value')
# ä¸Šä¸€è¡Œä»£ç åº”è¯¥è·Ÿè´Ÿæ ·æœ¬negæŸå¤±ä¸€æ ·å•Šï¼Œno_classesä¸fpmaskéƒ½æ˜¯æ•´å½¢æ ‡å¿—å•Š
# no_classesç›¸å½“äºæŠŠæ­£æ ·æœ¬å…¨ç½®ä¸º1ï¼Œè´Ÿæ ·æœ¬ä¸º0ï¼Œ
# åæ­£éƒ½æ˜¯ä»è´Ÿæ ·æœ¬ä¸­æŒ‰ç…§fnmaskæ ‡å¿—é€‰å‡ºç”¨äºå›å½’çš„è´Ÿæ ·æœ¬æŸå¤±å€¼ï¼Œè·Ÿç½®ä¸ç½®1æ²¡ä»€ä¹ˆå…³ç³»å•Š
```

é—®é¢˜å°±å‡ºåœ¨â€œæ­£æ ·æœ¬å…¨ç½®ä¸º1ï¼Œè´Ÿæ ·æœ¬ä¸º0â€è¿™å¥è¯ï¼Œä»£ç ä¸­`no_classes`æ˜¯æ¥è‡ª`pmask`ï¼Œè€Œ`pmask`æ¥è‡ª`gscores>0.5`ï¼Œè™½ç„¶`gscores`æ˜¯ä¸`gclasses`ä¸€ä¸€å¯¹åº”ï¼ˆæœ‰ç±»åˆ«å°±æœ‰åˆ†æ•°é™¤äº†èƒŒæ™¯0ï¼‰ï¼Œä½†æ˜¯`gscores>0.5`å°±ä¸ä¸€ä¸€å¯¹åº”äº†ï¼Œä¾‹å¦‚å¯èƒ½ï¼š[7ï¼Œ0ï¼Œ15ï¼Œ5]------IoU>0.5---->[1ï¼Œ0ï¼Œ0ï¼Œ0]ï¼Œè™½ç„¶æœ‰ç±»åˆ«15ï¼Œ5ï¼Œä½†IoU<0.5ï¼Œè¿˜æ˜¯GGã€‚

çŸ¥é“äº†æ­£æ ·æœ¬çš„å¯¹åº”å…³ç³»ï¼Œè´Ÿæ ·æœ¬çš„å°±å¥½ç†è§£äº†ï¼Œ`nmask`æ¥è‡ª`not pmask`ï¼Œå†ç»è¿‡æ’åºç­›é€‰å¾—å‡ºï¼š

[1ï¼Œ0ï¼Œ0ï¼Œ0]---not---->[0ï¼Œ1ï¼Œ1ï¼Œ1]--ç­›é€‰-->[0ï¼Œ1ï¼Œ0ï¼Œ1]ï¼Œæ‰€ä»¥ä¸èƒ½ç”¨gclassesè®¡ç®—è´Ÿæ ·æœ¬æŸå¤±ï¼Œå› ä¸ºæ‰“äº†ç±»åˆ«æ ‡ç­¾çš„ä¹Ÿä¸ä¸€å®šæ˜¯æ­£æ ·æœ¬ï¼Œè¯´ç™½äº†å°±æ˜¯å†2.3èŠ‚encode boxä¸­æ‰“é”™æ ‡ç­¾äº†ï¼ï¼ï¼ï¼æ‰€ä»¥è¿™ä¸ªå‘åœ¨è®¡ç®—æŸå¤±å‡½æ•°çš„æ—¶å€™æ¥å¡«ï¼Œå¯¹åº”ä¸Šé¢ç»™å‡ºçš„ç®€å•ä¾‹å­[7,0,15,5]ï¼š

+ æ­£æ ·æœ¬Lossï¼šfpmask[1,0,0,0]ï¼Œåªè®¡ç®—7çš„

+ è´Ÿæ ·æœ¬Lossï¼šfnmask[0,1,0,1]ï¼Œè®¡ç®—0å’Œ5çš„ï¼Œä½†è¿™æ—¶å€™ä¿®æ­£label=noclasses=[1,0,0,0]ï¼Œ
+ å¦‚æœæ˜¯æŒ‰ç…§æˆ‘æƒ³çš„ï¼Œfnmask[0,1,0,1]ï¼Œè®¡ç®—0å’Œ5çš„ï¼Œlabel = [7,0,15,5]ï¼Œé‚£ä¹ˆ5å¯¹åº”çš„æ ‡ç­¾å°±é”™äº†ï¼Œåº”è¯¥æ˜¯0

![SSD_Loss](../img/SSD_Loss.png)

#### 3.2.2è´Ÿæ ·æœ¬æ’åº

[Hard negative mining](https://www.zhihu.com/question/46292829)

> ç ”ç©¶äº†ä¸€ä¸‹ï¼Œå¸Œæœ›å¯¹ä½ æœ‰å¸®åŠ©ã€‚é¦–å…ˆæ˜¯negativeï¼Œå³è´Ÿæ ·æœ¬ï¼Œå…¶æ¬¡æ˜¯hardï¼Œè¯´æ˜æ˜¯å›°éš¾æ ·æœ¬ï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨å¯¹è´Ÿæ ·æœ¬åˆ†ç±»æ—¶å€™ï¼Œlossæ¯”è¾ƒå¤§ï¼ˆlabelä¸predictionç›¸å·®è¾ƒå¤§ï¼‰çš„é‚£äº›æ ·æœ¬ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯å®¹æ˜“å°†è´Ÿæ ·æœ¬çœ‹æˆæ­£æ ·æœ¬çš„é‚£äº›æ ·æœ¬ï¼Œä¾‹å¦‚roié‡Œæ²¡æœ‰ç‰©ä½“ï¼Œå…¨æ˜¯èƒŒæ™¯ï¼Œè¿™æ—¶å€™åˆ†ç±»å™¨å¾ˆå®¹æ˜“æ­£ç¡®åˆ†ç±»æˆèƒŒæ™¯ï¼Œè¿™ä¸ªå°±å«easy negativeï¼›å¦‚æœroié‡Œæœ‰äºŒåˆ†ä¹‹ä¸€ä¸ªç‰©ä½“ï¼Œæ ‡ç­¾ä»æ˜¯è´Ÿæ ·æœ¬ï¼Œè¿™æ—¶å€™åˆ†ç±»å™¨å°±å®¹æ˜“æŠŠä»–çœ‹æˆæ­£æ ·æœ¬ï¼Œè¿™æ—¶å€™å°±æ˜¯had negativeã€‚
> hard negative miningå°±æ˜¯å¤šæ‰¾ä¸€äº›hard negativeåŠ å…¥è´Ÿæ ·æœ¬é›†ï¼Œè¿›è¡Œè®­ç»ƒï¼Œè¿™æ ·ä¼šæ¯”easy negativeç»„æˆçš„è´Ÿæ ·æœ¬é›†æ•ˆæœæ›´å¥½ã€‚ä¸»è¦ä½“ç°åœ¨è™šè­¦ç‡æ›´ä½ä¸€äº›ï¼ˆä¹Ÿå°±æ˜¯false positiveå°‘ï¼‰ã€‚

æ‰€ä»¥æ’åºæ—¶å¹¶ä¸æ˜¯æŠŠé‚£äº›é¢„æµ‹èƒŒæ™¯ç±»åˆ«å¾—åˆ†é«˜çš„é‚£äº›ï¼Œè€Œæ˜¯å¾—åˆ†ä½çš„é‚£äº›ï¼Œè¯´æ˜ä½ ç½‘ç»œæ²¡çŒœå¯¹ï¼ŒHARDï¼

### 3.3Train

```python
g = tf.Graph()

# Create the model and specify the losses...
...

total_loss = slim.losses.get_total_loss()
optimizer = tf.train.GradientDescentOptimizer(learning_rate)

# create_train_op ensures that each time we ask for the loss, the update_ops
# are run and the gradients being computed are applied too.
train_op = slim.learning.create_train_op(total_loss, optimizer)
logdir = ... # Where checkpoints are stored.

slim.learning.train(
    train_op,
    logdir,
    number_of_steps=1000,
    save_summaries_secs=300,
    save_interval_secs=600):
```